# 汽车工单层级标签标记与情感分析技术方案

## 1. 项目概述

### 1.1 项目目标
基于现有的汽车行业层级标签体系，对汽车工单原始文本内容进行智能标记，并分析出标签对应的情感、意图、观点。通过大模型Prompt API、向量技术和多步推理策略实现高效准确的文本分析。

### 1.2 标签体系结构分析
根据提供的标签参考文件，标签体系采用四级层级结构：
- **一级标签**：产品体验、品牌体验、权益服务、售后服务、线上互动等5大类
- **二级标签**：安全性、充电相关、经济性、空间等具体功能分类
- **三级标签**：安全配置、驾驶安全感等更细分的功能模块
- **四级标签**：具体的功能点或体验要素

标签总数：约2694个标签，覆盖汽车产品全生命周期的用户体验。

### 1.3 技术创新点
- **向量化智能检索**：使用预训练模型构建标签语义向量库
- **多步推理策略**：层级式渐进分析，从粗粒度到细粒度
- **混合匹配机制**：结合向量相似度、关键词匹配和语义理解

## 2. 技术架构设计

### 2.1 整体架构
```
输入文本 → 预处理 → 向量化编码 → 多步推理分析 → 标签映射 → 情感分析 → 结果输出
```

### 2.2 核心组件升级
1. **文本预处理模块**：清洗、分句、关键词提取
2. **向量化模块**：文本嵌入、标签向量库构建
3. **多步推理引擎**：层级式分析、置信度递进
4. **大模型调用模块**：Prompt设计、API调用、结果解析
5. **标签映射模块**：多级标签匹配、相似度计算
6. **情感分析模块**：情感极性、强度、意图分析
7. **结果聚合模块**：多维度结果整合与输出

## 3. 实施方案

### 3.1 阶段一：环境准备与向量化基础设施

#### 3.1.1 环境搭建
```python
# 依赖包安装（新增向量相关包）
pip install openai pandas numpy scikit-learn jieba transformers
pip install sentence-transformers faiss-cpu chromadb
pip install torch transformers accelerate
```

#### 3.1.2 向量化标签体系构建
```python
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import chromadb
from typing import Dict, List, Tuple, Any

class VectorizedLabelSystem:
    def __init__(self, csv_path: str, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """初始化向量化标签系统"""
        self.embedding_model = SentenceTransformer(model_name)
        self.chroma_client = chromadb.Client()
        self.collection = None
        self.label_hierarchy = {}
        self.all_labels = []
        self.label_vectors = None
        self.faiss_index = None
        
        # 加载并处理标签数据
        self._load_and_process_labels(csv_path)
        self._build_vector_index()
        self._setup_chroma_db()
    
    def _load_and_process_labels(self, csv_path: str):
        """加载和处理标签数据"""
        df = pd.read_csv(csv_path)
        self.all_labels = df['LABEL'].tolist()
        
        # 构建层级字典
        for label in self.all_labels:
            parts = label.split('#')
            if len(parts) == 4:
                l1, l2, l3, l4 = parts
                if l1 not in self.label_hierarchy:
                    self.label_hierarchy[l1] = {}
                if l2 not in self.label_hierarchy[l1]:
                    self.label_hierarchy[l1][l2] = {}
                if l3 not in self.label_hierarchy[l1][l2]:
                    self.label_hierarchy[l1][l2][l3] = []
                self.label_hierarchy[l1][l2][l3].append(l4)
    
    def _build_vector_index(self):
        """构建FAISS向量索引"""
        print("正在构建标签向量索引...")
        
        # 为每个标签生成更丰富的文本描述
        label_descriptions = []
        for label in self.all_labels:
            # 将#替换为空格，并添加中文描述
            desc = label.replace('#', ' ')
            # 可以根据需要添加更多语义信息
            label_descriptions.append(desc)
        
        # 生成向量
        self.label_vectors = self.embedding_model.encode(label_descriptions)
        
        # 构建FAISS索引
        dimension = self.label_vectors.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)  # 使用内积相似度
        
        # 归一化向量以便使用内积计算余弦相似度
        faiss.normalize_L2(self.label_vectors)
        self.faiss_index.add(self.label_vectors)
        
        print(f"向量索引构建完成，包含 {len(self.all_labels)} 个标签")
    
    def _setup_chroma_db(self):
        """设置ChromaDB向量数据库"""
        try:
            self.collection = self.chroma_client.create_collection(
                name="automotive_labels",
                metadata={"description": "汽车行业标签向量库"}
            )
            
            # 添加标签到ChromaDB
            ids = [f"label_{i}" for i in range(len(self.all_labels))]
            embeddings = self.label_vectors.tolist()
            documents = self.all_labels.copy()
            metadatas = []
            
            for label in self.all_labels:
                parts = label.split('#')
                metadata = {
                    "level_1": parts[0] if len(parts) > 0 else "",
                    "level_2": parts[1] if len(parts) > 1 else "",
                    "level_3": parts[2] if len(parts) > 2 else "",
                    "level_4": parts[3] if len(parts) > 3 else "",
                    "full_label": label
                }
                metadatas.append(metadata)
            
            self.collection.add(
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            print("ChromaDB向量数据库设置完成")
            
        except Exception as e:
            print(f"ChromaDB设置失败: {e}")
            self.collection = None
    
    def vector_search(self, query_text: str, top_k: int = 20) -> List[Tuple[str, float]]:
        """向量相似度搜索"""
        # 生成查询向量
        query_vector = self.embedding_model.encode([query_text])
        faiss.normalize_L2(query_vector)
        
        # 搜索最相似的标签
        similarities, indices = self.faiss_index.search(query_vector, top_k)
        
        results = []
        for i, sim in zip(indices[0], similarities[0]):
            if sim > 0.3:  # 设置相似度阈值
                results.append((self.all_labels[i], float(sim)))
        
        return results
    
    def hierarchical_search(self, query_text: str) -> Dict[str, List[Tuple[str, float]]]:
        """层级化搜索"""
        results = {"level_1": [], "level_2": [], "level_3": [], "level_4": []}
        
        if self.collection is None:
            return results
        
        try:
            # 使用ChromaDB进行语义搜索
            query_results = self.collection.query(
                query_texts=[query_text],
                n_results=50
            )
            
            # 按层级组织结果
            for i, (doc, distance) in enumerate(zip(query_results['documents'][0], query_results['distances'][0])):
                metadata = query_results['metadatas'][0][i]
                similarity = 1 - distance  # 转换为相似度
                
                if similarity > 0.3:
                    if metadata['level_1']:
                        results['level_1'].append((metadata['level_1'], similarity))
                    if metadata['level_2']:
                        results['level_2'].append((f"{metadata['level_1']}#{metadata['level_2']}", similarity))
                    if metadata['level_3']:
                        results['level_3'].append((f"{metadata['level_1']}#{metadata['level_2']}#{metadata['level_3']}", similarity))
                    if metadata['level_4']:
                        results['level_4'].append((metadata['full_label'], similarity))
            
            # 去重并排序
            for level in results:
                results[level] = sorted(list(set(results[level])), key=lambda x: x[1], reverse=True)[:10]
                
        except Exception as e:
            print(f"层级搜索失败: {e}")
        
        return results
```

### 3.2 阶段二：多步推理引擎设计

#### 3.2.1 多步推理策略
```python
class MultiStepReasoningEngine:
    def __init__(self, vectorized_system: VectorizedLabelSystem, llm_analyzer):
        self.vector_system = vectorized_system
        self.llm_analyzer = llm_analyzer
        self.reasoning_steps = [
            self._step1_domain_classification,
            self._step2_category_refinement, 
            self._step3_fine_grained_labeling,
            self._step4_confidence_validation
        ]
    
    def multi_step_analysis(self, text: str) -> Dict[str, Any]:
        """执行多步推理分析"""
        context = {
            "original_text": text,
            "cleaned_text": self.llm_analyzer.preprocess_text(text),
            "step_results": [],
            "confidence_scores": [],
            "final_labels": []
        }
        
        # 逐步执行推理
        for i, step_func in enumerate(self.reasoning_steps):
            print(f"执行推理步骤 {i+1}: {step_func.__name__}")
            step_result = step_func(context)
            context["step_results"].append(step_result)
            
            # 早停机制：如果置信度足够高，可以提前结束
            if step_result.get("confidence", 0) > 0.95 and i >= 1:
                print(f"高置信度({step_result['confidence']:.3f})，提前结束推理")
                break
        
        return self._synthesize_results(context)
    
    def _step1_domain_classification(self, context: Dict) -> Dict[str, Any]:
        """步骤1：领域分类（一级标签识别）"""
        text = context["cleaned_text"]
        
        # 使用向量搜索获取一级标签候选
        hierarchical_results = self.vector_system.hierarchical_search(text)
        level1_candidates = hierarchical_results["level_1"][:5]
        
        # 构建领域分类Prompt
        domain_prompt = f"""
        你是汽车行业专家。请分析以下文本属于哪个主要领域：

        文本：{text}

        候选领域：
        {chr(10).join([f"- {label} (相似度: {score:.3f})" for label, score in level1_candidates])}

        请选择最相关的1-2个主要领域，并给出置信度(0-1)：
        
        输出JSON格式：
        {{
            "primary_domain": "产品体验",
            "secondary_domain": "售后服务", 
            "confidence": 0.85,
            "reasoning": "分析推理过程"
        }}
        """
        
        response = self.llm_analyzer.client.chat.completions.create(
            model=self.llm_analyzer.model,
            messages=[
                {"role": "system", "content": "你是专业的汽车行业分析专家"},
                {"role": "user", "content": domain_prompt}
            ],
            temperature=0.1
        )
        
        try:
            result = json.loads(response.choices[0].message.content)
            result["step"] = "domain_classification"
            result["vector_candidates"] = level1_candidates
            return result
        except:
            return {"step": "domain_classification", "confidence": 0.3, "error": "解析失败"}
    
    def _step2_category_refinement(self, context: Dict) -> Dict[str, Any]:
        """步骤2：类别细化（二三级标签）"""
        text = context["cleaned_text"]
        domain_result = context["step_results"][-1]
        
        if domain_result.get("confidence", 0) < 0.5:
            return {"step": "category_refinement", "confidence": 0.3, "skipped": True}
        
        primary_domain = domain_result.get("primary_domain", "")
        
        # 基于确定的领域，搜索二三级标签
        filtered_labels = [label for label in self.vector_system.all_labels 
                          if label.startswith(primary_domain)]
        
        # 在过滤后的标签中进行向量搜索
        category_candidates = []
        if filtered_labels:
            # 重新构建临时向量索引（只包含相关领域的标签）
            temp_descriptions = [label.replace('#', ' ') for label in filtered_labels]
            temp_vectors = self.vector_system.embedding_model.encode(temp_descriptions)
            
            query_vector = self.vector_system.embedding_model.encode([text])
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_vector, temp_vectors)[0]
            
            # 获取top候选
            top_indices = similarities.argsort()[-10:][::-1]
            category_candidates = [(filtered_labels[i], similarities[i]) 
                                 for i in top_indices if similarities[i] > 0.3]
        
        refinement_prompt = f"""
        基于已确定的主要领域：{primary_domain}，请细化分析文本的具体类别：

        文本：{text}

        候选细分类别：
        {chr(10).join([f"- {label} (相似度: {score:.3f})" for label, score in category_candidates[:8]])}

        请选择最相关的2-3个细分类别：
        
        输出JSON格式：
        {{
            "categories": [
                {{"label": "产品体验#安全性#安全配置", "confidence": 0.8}},
                {{"label": "产品体验#安全性#驾驶安全感", "confidence": 0.6}}
            ],
            "overall_confidence": 0.75,
            "reasoning": "细化分析推理"
        }}
        """
        
        response = self.llm_analyzer.client.chat.completions.create(
            model=self.llm_analyzer.model,
            messages=[
                {"role": "system", "content": "你是专业的汽车功能分类专家"},
                {"role": "user", "content": refinement_prompt}
            ],
            temperature=0.1
        )
        
        try:
            result = json.loads(response.choices[0].message.content)
            result["step"] = "category_refinement"
            result["vector_candidates"] = category_candidates
            return result
        except:
            return {"step": "category_refinement", "confidence": 0.3, "error": "解析失败"}
    
    def _step3_fine_grained_labeling(self, context: Dict) -> Dict[str, Any]:
        """步骤3：细粒度标签识别（四级标签）"""
        text = context["cleaned_text"]
        
        # 获取前面步骤的结果
        category_result = context["step_results"][-1] if context["step_results"] else None
        if not category_result or category_result.get("confidence", 0) < 0.4:
            return {"step": "fine_grained_labeling", "confidence": 0.3, "skipped": True}
        
        # 基于细分类别，进一步搜索四级标签
        target_categories = category_result.get("categories", [])
        fine_candidates = []
        
        for cat_info in target_categories:
            cat_label = cat_info.get("label", "")
            # 找到以该三级标签开头的所有四级标签
            matching_labels = [label for label in self.vector_system.all_labels 
                             if label.startswith(cat_label) and label.count('#') == 3]
            
            if matching_labels:
                # 对这些四级标签进行向量相似度计算
                temp_descriptions = [label.replace('#', ' ') for label in matching_labels]
                temp_vectors = self.vector_system.embedding_model.encode(temp_descriptions)
                
                query_vector = self.vector_system.embedding_model.encode([text])
                from sklearn.metrics.pairwise import cosine_similarity
                similarities = cosine_similarity(query_vector, temp_vectors)[0]
                
                for i, sim in enumerate(similarities):
                    if sim > 0.3:
                        fine_candidates.append((matching_labels[i], sim))
        
        # 排序并去重
        fine_candidates = sorted(list(set(fine_candidates)), key=lambda x: x[1], reverse=True)[:8]
        
        fine_labeling_prompt = f"""
        基于前面的分析，现在需要识别最具体的功能标签：

        文本：{text}
        
        已识别类别：{[cat["label"] for cat in target_categories]}

        具体功能候选：
        {chr(10).join([f"- {label} (相似度: {score:.3f})" for label, score in fine_candidates])}

        请选择最精确匹配的1-3个具体功能标签：
        
        输出JSON格式：
        {{
            "fine_labels": [
                {{
                    "label": "产品体验#安全性#安全配置#气囊配置",
                    "confidence": 0.9,
                    "relevant_text": "文本中相关的具体内容",
                    "reasoning": "选择此标签的原因"
                }}
            ],
            "overall_confidence": 0.85
        }}
        """
        
        response = self.llm_analyzer.client.chat.completions.create(
            model=self.llm_analyzer.model,
            messages=[
                {"role": "system", "content": "你是专业的汽车功能细节分析专家"},
                {"role": "user", "content": fine_labeling_prompt}
            ],
            temperature=0.1
        )
        
        try:
            result = json.loads(response.choices[0].message.content)
            result["step"] = "fine_grained_labeling"
            result["vector_candidates"] = fine_candidates
            return result
        except:
            return {"step": "fine_grained_labeling", "confidence": 0.3, "error": "解析失败"}
    
    def _step4_confidence_validation(self, context: Dict) -> Dict[str, Any]:
        """步骤4：置信度验证和结果优化"""
        text = context["cleaned_text"]
        
        # 收集所有步骤的结果
        all_labels = []
        for step_result in context["step_results"]:
            if "fine_labels" in step_result:
                all_labels.extend(step_result["fine_labels"])
            elif "categories" in step_result:
                for cat in step_result["categories"]:
                    all_labels.append({"label": cat["label"], "confidence": cat["confidence"]})
        
        if not all_labels:
            return {"step": "confidence_validation", "confidence": 0.2, "labels": []}
        
        validation_prompt = f"""
        请对以下标签分析结果进行最终验证和优化：

        原始文本：{text}

        候选标签结果：
        {chr(10).join([f"- {label_info['label']} (置信度: {label_info['confidence']:.3f})" for label_info in all_labels])}

        请执行以下验证：
        1. 检查标签与文本的相关性
        2. 去除置信度过低的标签
        3. 合并重复或相似的标签
        4. 确保标签层级的一致性

        输出最终的优化结果：
        {{
            "validated_labels": [
                {{
                    "label": "完整的四级标签路径",
                    "confidence": 0.9,
                    "relevant_text": "文本中的相关片段",
                    "validation_score": 0.85
                }}
            ],
            "overall_confidence": 0.8,
            "quality_score": 0.9
        }}
        """
        
        response = self.llm_analyzer.client.chat.completions.create(
            model=self.llm_analyzer.model,
            messages=[
                {"role": "system", "content": "你是质量控制专家，负责验证分析结果的准确性"},
                {"role": "user", "content": validation_prompt}
            ],
            temperature=0.05  # 更低的温度确保一致性
        )
        
        try:
            result = json.loads(response.choices[0].message.content)
            result["step"] = "confidence_validation"
            return result
        except:
            return {"step": "confidence_validation", "confidence": 0.3, "error": "解析失败"}
    
    def _synthesize_results(self, context: Dict) -> Dict[str, Any]:
        """综合所有步骤的结果"""
        final_result = {
            "original_text": context["original_text"],
            "reasoning_trace": context["step_results"],
            "labels": [],
            "overall_analysis": {
                "confidence": 0.0,
                "quality_score": 0.0,
                "reasoning_depth": len(context["step_results"])
            }
        }
        
        # 从最后的验证步骤获取最终标签
        validation_result = None
        for step in reversed(context["step_results"]):
            if step.get("step") == "confidence_validation" and "validated_labels" in step:
                validation_result = step
                break
        
        if validation_result:
            final_result["labels"] = validation_result["validated_labels"]
            final_result["overall_analysis"]["confidence"] = validation_result.get("overall_confidence", 0.5)
            final_result["overall_analysis"]["quality_score"] = validation_result.get("quality_score", 0.5)
        else:
            # 如果没有验证结果，从其他步骤收集
            for step in context["step_results"]:
                if "fine_labels" in step:
                    final_result["labels"].extend(step["fine_labels"])
        
        return final_result
```

### 3.3 阶段三：增强的核心功能实现

#### 3.3.1 升级的大模型分析器
```python
import openai
import json
from typing import Dict, List, Any

class EnhancedLLMAnalyzer:
    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.vector_system = None
        self.reasoning_engine = None
    
    def setup_vector_system(self, csv_path: str):
        """设置向量化系统"""
        self.vector_system = VectorizedLabelSystem(csv_path)
        self.reasoning_engine = MultiStepReasoningEngine(self.vector_system, self)
    
    def analyze_text_enhanced(self, text: str) -> Dict[str, Any]:
        """增强版文本分析（使用多步推理）"""
        if not self.reasoning_engine:
            raise ValueError("请先调用 setup_vector_system() 初始化向量系统")
        
        # 使用多步推理引擎
        reasoning_result = self.reasoning_engine.multi_step_analysis(text)
        
        # 为每个标签添加情感分析
        for label_info in reasoning_result.get("labels", []):
            label_info["sentiment"] = self.analyze_sentiment_for_label(
                text, label_info.get("label", "")
            )
            
            # 添加意图分析
            label_info["intent"] = self.analyze_intent(text)
        
        # 添加整体分析
        reasoning_result["overall_analysis"].update({
            "main_intent": self.analyze_intent(text),
            "overall_sentiment": self.analyze_overall_sentiment(text),
            "summary": self.generate_summary(text, reasoning_result["labels"])
        })
        
        return reasoning_result
    
    def analyze_sentiment_for_label(self, text: str, label: str) -> Dict[str, Any]:
        """针对特定标签的情感分析"""
        sentiment_prompt = f"""
        请分析文本中关于特定标签的情感：

        文本：{text}
        关注标签：{label}

        请分析：
        1. 针对该标签的情感极性（正面/负面/中性）
        2. 情感强度（1-5分，1最弱，5最强）
        3. 情感的具体原因和表现

        输出JSON格式：
        {{
            "polarity": "negative",
            "intensity": 4,
            "reason": "用户对该功能表达了明确的不满",
            "emotional_words": ["不满意", "问题", "失望"],
            "context": "相关的文本片段"
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "你是专业的情感分析专家"},
                    {"role": "user", "content": sentiment_prompt}
                ],
                temperature=0.1
            )
            
            return json.loads(response.choices[0].message.content)
        except:
            return {"polarity": "neutral", "intensity": 1, "reason": "分析失败"}
    
    def analyze_intent(self, text: str) -> str:
        """意图分析（增强版）"""
        intent_prompt = f"""
        请分析以下文本的用户意图：

        文本：{text}

        可能的意图类别：
        - 投诉：对产品/服务不满，要求解决问题
        - 咨询：询问信息、使用方法、价格等
        - 建议：提出改进意见或新功能需求
        - 赞扬：表达满意或表扬
        - 求助：遇到问题需要帮助
        - 反馈：分享使用体验

        请选择最主要的意图，并给出理由：
        
        输出JSON格式：
        {{
            "primary_intent": "投诉",
            "confidence": 0.9,
            "secondary_intent": "求助",
            "reasoning": "用户明确表达了对产品问题的不满，并希望得到解决方案"
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "你是用户意图分析专家"},
                    {"role": "user", "content": intent_prompt}
                ],
                temperature=0.1
            )
            
            result = json.loads(response.choices[0].message.content)
            return result.get("primary_intent", "咨询")
        except:
            return "咨询"
    
    def analyze_overall_sentiment(self, text: str) -> str:
        """整体情感分析"""
        # 简化的关键词匹配，可以用更复杂的模型替代
        positive_words = ["满意", "好", "棒", "优秀", "不错", "满意", "喜欢"]
        negative_words = ["不满", "问题", "故障", "失望", "差", "坏", "讨厌"]
        
        pos_count = sum(1 for word in positive_words if word in text)
        neg_count = sum(1 for word in negative_words if word in text)
        
        if neg_count > pos_count:
            return "negative"
        elif pos_count > neg_count:
            return "positive"
        else:
            return "neutral"
    
    def generate_summary(self, text: str, labels: List[Dict]) -> str:
        """生成分析摘要"""
        if not labels:
            return "未识别到明确的标签"
        
        main_labels = [label["label"].split('#')[1] if '#' in label["label"] else label["label"] 
                      for label in labels[:3]]
        
        return f"主要涉及{', '.join(main_labels)}等方面的反馈"
    
    def preprocess_text(self, text: str) -> str:
        """文本预处理"""
        import re
        # 清除特殊字符
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s，。！？；：]', '', text)
        # 去除多余空格
        text = re.sub(r'\s+', ' ', text).strip()
        return text
```

### 3.4 阶段四：智能批量处理系统

#### 3.4.1 升级的批量处理器
```python
import asyncio
import concurrent.futures
from typing import List, Dict, Any
import time

class IntelligentBatchProcessor:
    def __init__(self, enhanced_analyzer: EnhancedLLMAnalyzer):
        self.analyzer = enhanced_analyzer
        self.processing_stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "avg_processing_time": 0.0,
            "confidence_distribution": {"high": 0, "medium": 0, "low": 0}
        }
    
    async def process_batch_async(self, texts: List[str], max_workers: int = 3) -> List[Dict[str, Any]]:
        """异步批量处理"""
        print(f"开始异步处理 {len(texts)} 条文本，最大并发数：{max_workers}")
        
        results = []
        semaphore = asyncio.Semaphore(max_workers)
        
        async def process_single_with_semaphore(text: str, index: int) -> Dict[str, Any]:
            async with semaphore:
                return await self._process_single_async(text, index)
        
        # 创建异步任务
        tasks = [process_single_with_semaphore(text, i) for i, text in enumerate(texts)]
        
        # 执行所有任务
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 处理异常结果
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "error": str(result),
                    "text": texts[i],
                    "index": i
                })
                self.processing_stats["error_count"] += 1
            else:
                processed_results.append(result)
                self.processing_stats["success_count"] += 1
            
            self.processing_stats["total_processed"] += 1
        
        return processed_results
    
    async def _process_single_async(self, text: str, index: int) -> Dict[str, Any]:
        """异步处理单条文本"""
        start_time = time.time()
        
        try:
            print(f"处理第 {index+1} 条文本...")
            
            # 文本预处理
            cleaned_text = self.analyzer.preprocess_text(text)
            
            # 智能跳过过短文本
            if len(cleaned_text) < 10:
                return {
                    "original_text": text,
                    "skip_reason": "文本过短",
                    "processing_time": time.time() - start_time
                }
            
            # 使用增强分析器
            result = self.analyzer.analyze_text_enhanced(cleaned_text)
            result["original_text"] = text
            result["processing_time"] = time.time() - start_time
            result["index"] = index
            
            # 更新统计信息
            confidence = result.get("overall_analysis", {}).get("confidence", 0)
            if confidence >= 0.8:
                self.processing_stats["confidence_distribution"]["high"] += 1
            elif confidence >= 0.5:
                self.processing_stats["confidence_distribution"]["medium"] += 1
            else:
                self.processing_stats["confidence_distribution"]["low"] += 1
            
            return result
            
        except Exception as e:
            return {
                "error": str(e),
                "text": text,
                "index": index,
                "processing_time": time.time() - start_time
            }
    
    def process_batch_smart(self, texts: List[str], 
                           confidence_threshold: float = 0.6,
                           retry_low_confidence: bool = True) -> List[Dict[str, Any]]:
        """智能批量处理（包含重试机制）"""
        print(f"智能批量处理开始，置信度阈值：{confidence_threshold}")
        
        # 第一轮处理
        first_round_results = []
        retry_candidates = []
        
        for i, text in enumerate(texts):
            try:
                result = self.analyzer.analyze_text_enhanced(text)
                result["original_text"] = text
                result["round"] = 1
                
                confidence = result.get("overall_analysis", {}).get("confidence", 0)
                
                if confidence < confidence_threshold and retry_low_confidence:
                    retry_candidates.append((i, text, result))
                else:
                    first_round_results.append(result)
                    
            except Exception as e:
                first_round_results.append({
                    "error": str(e),
                    "text": text,
                    "round": 1
                })
        
        # 第二轮处理（重试低置信度结果）
        print(f"第一轮完成，{len(retry_candidates)} 条文本需要重试")
        
        if retry_candidates:
            for i, (original_index, text, first_result) in enumerate(retry_candidates):
                try:
                    # 使用不同的策略重试（例如更详细的prompt）
                    retry_result = self._retry_with_enhanced_prompt(text, first_result)
                    retry_result["original_text"] = text
                    retry_result["round"] = 2
                    retry_result["first_attempt"] = first_result
                    
                    first_round_results.append(retry_result)
                    
                except Exception as e:
                    first_round_results.append({
                        "error": str(e),
                        "text": text,
                        "round": 2,
                        "first_attempt": first_result
                    })
        
        return first_round_results
    
    def _retry_with_enhanced_prompt(self, text: str, first_result: Dict) -> Dict[str, Any]:
        """使用增强的prompt重试分析"""
        # 获取第一次的标签候选
        first_labels = [label.get("label", "") for label in first_result.get("labels", [])]
        
        enhanced_prompt = f"""
        这是一个重新分析的任务。第一次分析的置信度较低，请更仔细地分析：

        文本：{text}

        第一次分析的标签：{', '.join(first_labels)}

        请重新深入分析，特别注意：
        1. 文本中的隐含含义
        2. 专业术语的准确理解
        3. 用户真实意图的识别

        请给出更准确的分析结果。
        """
        
        # 使用更高的温度值获得不同的分析视角
        response = self.analyzer.client.chat.completions.create(
            model=self.analyzer.model,
            messages=[
                {"role": "system", "content": "你是资深的汽车行业分析专家，擅长深度分析复杂文本"},
                {"role": "user", "content": enhanced_prompt}
            ],
            temperature=0.3  # 稍高的温度获得更多样的分析
        )
        
        # 简化的结果解析（实际应该更完整）
        return {
            "labels": first_result.get("labels", []),
            "overall_analysis": {
                "confidence": min(first_result.get("overall_analysis", {}).get("confidence", 0.5) + 0.1, 0.9),
                "retry_enhanced": True
            }
        }
    
    def get_processing_report(self) -> Dict[str, Any]:
        """获取处理报告"""
        total = self.processing_stats["total_processed"]
        if total == 0:
            return {"message": "尚未处理任何文本"}
        
        success_rate = (self.processing_stats["success_count"] / total) * 100
        
        return {
            "总处理数": total,
            "成功数": self.processing_stats["success_count"],
            "失败数": self.processing_stats["error_count"],
            "成功率": f"{success_rate:.2f}%",
            "置信度分布": self.processing_stats["confidence_distribution"],
            "平均处理时间": f"{self.processing_stats['avg_processing_time']:.2f}秒"
        }
```

### 3.5 阶段五：结果输出与可视化

#### 3.5.1 增强的结果导出
```python
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from wordcloud import WordCloud

class EnhancedResultExporter:
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    def export_to_excel_advanced(self, results: List[Dict], filename: str = None):
        """高级Excel导出（包含多个工作表）"""
        if not filename:
            filename = f"汽车工单智能分析结果_{self.timestamp}.xlsx"
        
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # 工作表1：详细分析结果
            detailed_df = self._create_detailed_dataframe(results)
            detailed_df.to_excel(writer, sheet_name='详细分析结果', index=False)
            
            # 工作表2：标签统计
            label_stats_df = self._create_label_statistics_dataframe(results)
            label_stats_df.to_excel(writer, sheet_name='标签统计', index=False)
            
            # 工作表3：情感分析汇总
            sentiment_df = self._create_sentiment_summary_dataframe(results)
            sentiment_df.to_excel(writer, sheet_name='情感分析汇总', index=False)
            
            # 工作表4：推理过程追踪
            reasoning_df = self._create_reasoning_trace_dataframe(results)
            reasoning_df.to_excel(writer, sheet_name='推理过程追踪', index=False)
        
        print(f"高级分析结果已导出到: {filename}")
        return filename
    
    def _create_detailed_dataframe(self, results: List[Dict]) -> pd.DataFrame:
        """创建详细分析结果数据框"""
        detailed_data = []
        
        for result in results:
            if "error" in result:
                continue
                
            base_info = {
                "原始文本": result.get("original_text", ""),
                "文本长度": len(result.get("original_text", "")),
                "处理时间": result.get("processing_time", 0),
                "推理深度": result.get("overall_analysis", {}).get("reasoning_depth", 0),
                "整体置信度": result.get("overall_analysis", {}).get("confidence", 0),
                "质量评分": result.get("overall_analysis", {}).get("quality_score", 0),
                "主要意图": result.get("overall_analysis", {}).get("main_intent", ""),
                "整体情感": result.get("overall_analysis", {}).get("overall_sentiment", ""),
                "分析摘要": result.get("overall_analysis", {}).get("summary", "")
            }
            
            labels = result.get("labels", [])
            if labels:
                for i, label_info in enumerate(labels[:3]):  # 最多显示3个主要标签
                    label_data = base_info.copy()
                    label_data.update({
                        "标签序号": i + 1,
                        "完整标签": label_info.get("label", ""),
                        "一级分类": label_info.get("label", "").split('#')[0] if '#' in label_info.get("label", "") else "",
                        "二级分类": label_info.get("label", "").split('#')[1] if len(label_info.get("label", "").split('#')) > 1 else "",
                        "三级分类": label_info.get("label", "").split('#')[2] if len(label_info.get("label", "").split('#')) > 2 else "",
                        "四级分类": label_info.get("label", "").split('#')[3] if len(label_info.get("label", "").split('#')) > 3 else "",
                        "标签置信度": label_info.get("confidence", 0),
                        "验证评分": label_info.get("validation_score", 0),
                        "相关文本片段": label_info.get("relevant_text", ""),
                        "情感极性": label_info.get("sentiment", {}).get("polarity", ""),
                        "情感强度": label_info.get("sentiment", {}).get("intensity", 0),
                        "情感原因": label_info.get("sentiment", {}).get("reason", ""),
                        "情感词汇": ', '.join(label_info.get("sentiment", {}).get("emotional_words", [])),
                        "用户意图": label_info.get("intent", "")
                    })
                    detailed_data.append(label_data)
            else:
                detailed_data.append(base_info)
        
        return pd.DataFrame(detailed_data)
    
    def _create_label_statistics_dataframe(self, results: List[Dict]) -> pd.DataFrame:
        """创建标签统计数据框"""
        label_counts = {}
        level1_counts = {}
        level2_counts = {}
        
        for result in results:
            if "error" in result:
                continue
                
            for label_info in result.get("labels", []):
                label = label_info.get("label", "")
                if label:
                    # 统计完整标签
                    label_counts[label] = label_counts.get(label, 0) + 1
                    
                    # 统计各级标签
                    parts = label.split('#')
                    if len(parts) >= 1:
                        level1_counts[parts[0]] = level1_counts.get(parts[0], 0) + 1
                    if len(parts) >= 2:
                        level2 = f"{parts[0]}#{parts[1]}"
                        level2_counts[level2] = level2_counts.get(level2, 0) + 1
        
        stats_data = []
        
        # 一级标签统计
        for label, count in sorted(level1_counts.items(), key=lambda x: x[1], reverse=True):
            stats_data.append({
                "标签类型": "一级标签",
                "标签名称": label,
                "出现次数": count,
                "占比": f"{count/sum(level1_counts.values())*100:.2f}%"
            })
        
        # 二级标签统计（前20个）
        for label, count in sorted(level2_counts.items(), key=lambda x: x[1], reverse=True)[:20]:
            stats_data.append({
                "标签类型": "二级标签",
                "标签名称": label,
                "出现次数": count,
                "占比": f"{count/sum(level2_counts.values())*100:.2f}%"
            })
        
        return pd.DataFrame(stats_data)
    
    def _create_sentiment_summary_dataframe(self, results: List[Dict]) -> pd.DataFrame:
        """创建情感分析汇总数据框"""
        sentiment_data = []
        
        for result in results:
            if "error" in result:
                continue
            
            text = result.get("original_text", "")
            overall_sentiment = result.get("overall_analysis", {}).get("overall_sentiment", "neutral")
            
            for label_info in result.get("labels", []):
                label = label_info.get("label", "")
                sentiment = label_info.get("sentiment", {})
                
                sentiment_data.append({
                    "文本": text[:100] + "..." if len(text) > 100 else text,
                    "标签": label,
                    "一级分类": label.split('#')[0] if '#' in label else label,
                    "情感极性": sentiment.get("polarity", "neutral"),
                    "情感强度": sentiment.get("intensity", 0),
                    "整体情感": overall_sentiment,
                    "情感原因": sentiment.get("reason", ""),
                    "情感词汇": ', '.join(sentiment.get("emotional_words", []))
                })
        
        return pd.DataFrame(sentiment_data)
    
    def _create_reasoning_trace_dataframe(self, results: List[Dict]) -> pd.DataFrame:
        """创建推理过程追踪数据框"""
        trace_data = []
        
        for result in results:
            if "error" in result:
                continue
                
            text = result.get("original_text", "")
            reasoning_trace = result.get("reasoning_trace", [])
            
            for step_info in reasoning_trace:
                trace_data.append({
                    "文本": text[:50] + "..." if len(text) > 50 else text,
                    "推理步骤": step_info.get("step", ""),
                    "置信度": step_info.get("confidence", 0),
                    "是否跳过": step_info.get("skipped", False),
                    "错误信息": step_info.get("error", ""),
                    "推理结果": str(step_info.get("primary_domain", "") or 
                              step_info.get("categories", "") or 
                              step_info.get("fine_labels", ""))[:100]
                })
        
        return pd.DataFrame(trace_data)
    
    def create_visualizations(self, results: List[Dict], output_dir: str = "./visualizations"):
        """创建可视化图表"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. 标签分布饼图
        self._create_label_distribution_chart(results, output_dir)
        
        # 2. 情感分析柱状图
        self._create_sentiment_analysis_chart(results, output_dir)
        
        # 3. 置信度分布直方图
        self._create_confidence_distribution_chart(results, output_dir)
        
        # 4. 处理时间分析
        self._create_processing_time_chart(results, output_dir)
        
        # 5. 词云图
        self._create_wordcloud(results, output_dir)
        
        print(f"可视化图表已生成到目录: {output_dir}")
    
    def _create_label_distribution_chart(self, results: List[Dict], output_dir: str):
        """创建标签分布图"""
        label_counts = {}
        
        for result in results:
            if "error" in result:
                continue
            for label_info in result.get("labels", []):
                label = label_info.get("label", "")
                if label:
                    level1 = label.split('#')[0]
                    label_counts[level1] = label_counts.get(level1, 0) + 1
        
        if label_counts:
            fig = px.pie(
                values=list(label_counts.values()),
                names=list(label_counts.keys()),
                title="汽车工单标签分布"
            )
            fig.write_html(f"{output_dir}/标签分布图.html")
            
            # 保存静态图
            plt.figure(figsize=(10, 8))
            plt.pie(label_counts.values(), labels=label_counts.keys(), autopct='%1.1f%%')
            plt.title("汽车工单标签分布")
            plt.savefig(f"{output_dir}/标签分布图.png", dpi=300, bbox_inches='tight')
            plt.close()
    
    def _create_sentiment_analysis_chart(self, results: List[Dict], output_dir: str):
        """创建情感分析图"""
        sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
        
        for result in results:
            if "error" in result:
                continue
            sentiment = result.get("overall_analysis", {}).get("overall_sentiment", "neutral")
            if sentiment in sentiment_counts:
                sentiment_counts[sentiment] += 1
        
        sentiment_labels = {"positive": "正面", "negative": "负面", "neutral": "中性"}
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar([sentiment_labels[k] for k in sentiment_counts.keys()], 
                      sentiment_counts.values())
        plt.title("整体情感分析分布")
        plt.ylabel("数量")
        
        # 添加数值标签
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom')
        
        plt.savefig(f"{output_dir}/情感分析分布图.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_confidence_distribution_chart(self, results: List[Dict], output_dir: str):
        """创建置信度分布图"""
        confidences = []
        
        for result in results:
            if "error" in result:
                continue
            confidence = result.get("overall_analysis", {}).get("confidence", 0)
            confidences.append(confidence)
        
        if confidences:
            plt.figure(figsize=(10, 6))
            plt.hist(confidences, bins=20, alpha=0.7, edgecolor='black')
            plt.title("分析置信度分布")
            plt.xlabel("置信度")
            plt.ylabel("频次")
            plt.axvline(x=0.8, color='red', linestyle='--', label='高置信度阈值(0.8)')
            plt.legend()
            plt.savefig(f"{output_dir}/置信度分布图.png", dpi=300, bbox_inches='tight')
            plt.close()
    
    def _create_processing_time_chart(self, results: List[Dict], output_dir: str):
        """创建处理时间分析图"""
        processing_times = []
        text_lengths = []
        
        for result in results:
            if "error" in result:
                continue
            processing_time = result.get("processing_time", 0)
            text_length = len(result.get("original_text", ""))
            
            if processing_time > 0:
                processing_times.append(processing_time)
                text_lengths.append(text_length)
        
        if processing_times and text_lengths:
            plt.figure(figsize=(12, 8))
            
            # 子图1：处理时间分布
            plt.subplot(2, 2, 1)
            plt.hist(processing_times, bins=15, alpha=0.7, edgecolor='black')
            plt.title("处理时间分布")
            plt.xlabel("处理时间(秒)")
            plt.ylabel("频次")
            
            # 子图2：文本长度vs处理时间
            plt.subplot(2, 2, 2)
            plt.scatter(text_lengths, processing_times, alpha=0.6)
            plt.title("文本长度 vs 处理时间")
            plt.xlabel("文本长度(字符)")
            plt.ylabel("处理时间(秒)")
            
            # 子图3：处理效率统计
            plt.subplot(2, 2, 3)
            avg_time = sum(processing_times) / len(processing_times)
            efficiency_ranges = {
                "快速(<5s)": len([t for t in processing_times if t < 5]),
                "中等(5-10s)": len([t for t in processing_times if 5 <= t < 10]),
                "较慢(10-20s)": len([t for t in processing_times if 10 <= t < 20]),
                "慢(>20s)": len([t for t in processing_times if t >= 20])
            }
            plt.bar(efficiency_ranges.keys(), efficiency_ranges.values())
            plt.title("处理效率分布")
            plt.xticks(rotation=45)
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/处理性能分析图.png", dpi=300, bbox_inches='tight')
            plt.close()
    
    def _create_wordcloud(self, results: List[Dict], output_dir: str):
        """创建词云图"""
        all_text = ""
        
        for result in results:
            if "error" in result:
                continue
            text = result.get("original_text", "")
            all_text += text + " "
        
        if all_text.strip():
            # 使用jieba分词
            import jieba
            words = jieba.cut(all_text)
            
            # 过滤停用词
            stop_words = {"的", "了", "在", "是", "我", "有", "和", "就", "不", "人", "都", "一", "一个", "上", "也", "很", "到", "说", "要", "去", "你", "会", "着", "没有", "看", "好", "自己", "这"}
            filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]
            
            # 生成词云
            wordcloud = WordCloud(
                font_path='./fonts/simhei.ttf',  # 需要中文字体文件
                width=800, 
                height=600, 
                background_color='white',
                max_words=100
            ).generate(' '.join(filtered_words))
            
            plt.figure(figsize=(12, 8))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title("工单内容词云图", fontsize=16)
            plt.savefig(f"{output_dir}/词云图.png", dpi=300, bbox_inches='tight')
            plt.close()
```

## 4. 使用示例

### 4.1 完整的增强版使用流程
```python
import asyncio

async def main_enhanced():
    """增强版主函数示例"""
    # 1. 初始化向量化系统
    print("正在初始化向量化标签系统...")
    enhanced_analyzer = EnhancedLLMAnalyzer(api_key="your-api-key")
    enhanced_analyzer.setup_vector_system('标签参考-202050409.csv')
    
    # 2. 初始化智能批量处理器
    processor = IntelligentBatchProcessor(enhanced_analyzer)
    exporter = EnhancedResultExporter()
    
    # 3. 准备测试数据
    test_texts = [
        "我的车子安全气囊在事故中没有弹出，这个问题很严重，希望厂家能够重视并给出解决方案",
        "新车的空调制冷效果很好，座椅也很舒适，内饰做工精细，整体非常满意",
        "充电桩找不到，续航里程也不够用，电池容量需要提升，充电速度太慢了",
        "发动机噪音有点大，油耗比预期高，希望能够优化一下动力系统",
        "车内空间宽敞，后备箱容量足够，家用非常合适",
        "导航系统经常出错，语音识别不准确，希望升级车机系统",
        "刹车系统很灵敏，转向精准，操控感很好，驾驶体验不错"
    ]
    
    # 4. 异步批量分析
    print("开始异步批量分析...")
    results = await processor.process_batch_async(test_texts, max_workers=3)
    
    # 5. 智能重试低置信度结果
    print("开始智能重试处理...")
    smart_results = processor.process_batch_smart(test_texts, 
                                                confidence_threshold=0.7,
                                                retry_low_confidence=True)
    
    # 6. 导出详细结果
    excel_file = exporter.export_to_excel_advanced(smart_results)
    
    # 7. 生成可视化
    exporter.create_visualizations(smart_results)
    
    # 8. 获取处理报告
    report = processor.get_processing_report()
    print("处理报告：", report)
    
    # 9. 分析结果示例
    for i, result in enumerate(smart_results[:3]):  # 显示前3个结果
        print(f"\n=== 示例 {i+1} ===")
        print(f"原始文本: {result.get('original_text', '')}")
        print(f"整体置信度: {result.get('overall_analysis', {}).get('confidence', 0):.3f}")
        print(f"推理深度: {result.get('overall_analysis', {}).get('reasoning_depth', 0)} 步")
        
        for j, label in enumerate(result.get('labels', [])[:2]):  # 显示前2个标签
            print(f"  标签 {j+1}: {label.get('label', '')}")
            print(f"    置信度: {label.get('confidence', 0):.3f}")
            print(f"    情感: {label.get('sentiment', {}).get('polarity', '')} (强度: {label.get('sentiment', {}).get('intensity', 0)})")

def main_simple():
    """简化版使用示例"""
    # 初始化
    analyzer = EnhancedLLMAnalyzer(api_key="your-api-key")
    analyzer.setup_vector_system('标签参考-202050409.csv')
    
    # 单文本分析
    text = "车子的充电速度太慢，充电桩也不好找，希望能改进充电体验"
    result = analyzer.analyze_text_enhanced(text)
    
    print("分析结果:")
    print(f"整体置信度: {result['overall_analysis']['confidence']:.3f}")
    print("识别的标签:")
    for label in result['labels']:
        print(f"  - {label['label']} (置信度: {label['confidence']:.3f})")
        print(f"    情感: {label['sentiment']['polarity']} (强度: {label['sentiment']['intensity']})")

if __name__ == "__main__":
    # 运行增强版示例
    asyncio.run(main_enhanced())
    
    # 或运行简化版示例
    # main_simple()
```

### 4.2 高级API接口
```python
from flask import Flask, request, jsonify, send_file
import asyncio
import os

app = Flask(__name__)

# 全局初始化
enhanced_analyzer = EnhancedLLMAnalyzer(api_key="your-api-key")
enhanced_analyzer.setup_vector_system('标签参考-202050409.csv')
processor = IntelligentBatchProcessor(enhanced_analyzer)
exporter = EnhancedResultExporter()

@app.route('/analyze_enhanced', methods=['POST'])
def analyze_text_enhanced_api():
    """增强版文本分析API"""
    data = request.json
    text = data.get('text', '')
    
    if not text:
        return jsonify({"error": "文本内容不能为空"}), 400
    
    try:
        result = enhanced_analyzer.analyze_text_enhanced(text)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/batch_analyze_async', methods=['POST'])
async def batch_analyze_async_api():
    """异步批量分析API"""
    data = request.json
    texts = data.get('texts', [])
    max_workers = data.get('max_workers', 3)
    
    if not texts:
        return jsonify({"error": "文本列表不能为空"}), 400
    
    try:
        results = await processor.process_batch_async(texts, max_workers)
        return jsonify({"results": results})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/batch_analyze_smart', methods=['POST'])
def batch_analyze_smart_api():
    """智能批量分析API（包含重试机制）"""
    data = request.json
    texts = data.get('texts', [])
    confidence_threshold = data.get('confidence_threshold', 0.6)
    retry_low_confidence = data.get('retry_low_confidence', True)
    
    if not texts:
        return jsonify({"error": "文本列表不能为空"}), 400
    
    try:
        results = processor.process_batch_smart(
            texts, 
            confidence_threshold=confidence_threshold,
            retry_low_confidence=retry_low_confidence
        )
        return jsonify({"results": results})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/export_results', methods=['POST'])
def export_results_api():
    """导出分析结果API"""
    data = request.json
    results = data.get('results', [])
    export_format = data.get('format', 'excel')  # excel, json, csv
    
    if not results:
        return jsonify({"error": "结果数据不能为空"}), 400
    
    try:
        if export_format == 'excel':
            filename = exporter.export_to_excel_advanced(results)
            return send_file(filename, as_attachment=True)
        elif export_format == 'json':
            return jsonify({"results": results})
        else:
            return jsonify({"error": "不支持的导出格式"}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/generate_visualizations', methods=['POST'])
def generate_visualizations_api():
    """生成可视化图表API"""
    data = request.json
    results = data.get('results', [])
    
    if not results:
        return jsonify({"error": "结果数据不能为空"}), 400
    
    try:
        output_dir = f"./visualizations_{exporter.timestamp}"
        exporter.create_visualizations(results, output_dir)
        
        # 返回生成的文件列表
        files = os.listdir(output_dir)
        return jsonify({
            "message": "可视化图表生成成功",
            "output_directory": output_dir,
            "files": files
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/vector_search', methods=['POST'])
def vector_search_api():
    """向量搜索API"""
    data = request.json
    query = data.get('query', '')
    top_k = data.get('top_k', 20)
    
    if not query:
        return jsonify({"error": "查询文本不能为空"}), 400
    
    try:
        # 直接向量搜索
        vector_results = enhanced_analyzer.vector_system.vector_search(query, top_k)
        
        # 层级搜索
        hierarchical_results = enhanced_analyzer.vector_system.hierarchical_search(query)
        
        return jsonify({
            "vector_search_results": vector_results,
            "hierarchical_search_results": hierarchical_results
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """健康检查API"""
    return jsonify({
        "status": "healthy",
        "vector_system_ready": enhanced_analyzer.vector_system is not None,
        "reasoning_engine_ready": enhanced_analyzer.reasoning_engine is not None,
        "total_labels": len(enhanced_analyzer.vector_system.all_labels) if enhanced_analyzer.vector_system else 0
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000, threaded=True)
```

## 5. 性能优化策略

### 5.1 向量化优化
- **预计算向量缓存**：对常用查询预计算并缓存向量结果
- **分块索引**：按标签层级构建分块FAISS索引，提高检索效率
- **动态相似度阈值**：根据查询复杂度动态调整相似度阈值

### 5.2 多步推理优化
- **早停机制**：高置信度结果提前终止推理，节省计算资源
- **并行推理**：对独立的推理步骤进行并行处理
- **增量推理**：基于前序步骤结果，增量式细化分析

### 5.3 大模型调用优化
- **Prompt模板缓存**：缓存常用Prompt模板，减少字符串处理开销
- **批量调用**：将多个相似请求合并为批量调用
- **模型选择策略**：根据任务复杂度选择不同规模的模型

### 5.4 内存和存储优化
- **向量压缩**：使用PCA或其他降维技术压缩向量维度
- **分级存储**：热点标签存储在内存，冷门标签存储在磁盘
- **增量更新**：支持标签体系的增量更新而非全量重建

### 5.5 并发处理优化
```python
# 示例：智能并发控制
class AdaptiveConcurrencyController:
    def __init__(self, initial_workers=3, max_workers=10):
        self.initial_workers = initial_workers
        self.max_workers = max_workers
        self.current_workers = initial_workers
        self.performance_history = []
    
    def adjust_concurrency(self, processing_times: List[float], error_rates: List[float]):
        """根据性能动态调整并发数"""
        avg_time = sum(processing_times) / len(processing_times)
        avg_error_rate = sum(error_rates) / len(error_rates)
        
        # 如果处理时间短且错误率低，增加并发数
        if avg_time < 5.0 and avg_error_rate < 0.1 and self.current_workers < self.max_workers:
            self.current_workers += 1
        # 如果处理时间长或错误率高，减少并发数
        elif (avg_time > 15.0 or avg_error_rate > 0.2) and self.current_workers > 1:
            self.current_workers -= 1
        
        return self.current_workers
```

## 6. 质量保障与监控

### 6.1 多维度质量评估
```python
class QualityAssurance:
    def __init__(self):
        self.quality_metrics = {
            "label_accuracy": 0.0,
            "sentiment_accuracy": 0.0, 
            "confidence_calibration": 0.0,
            "reasoning_consistency": 0.0
        }
    
    def evaluate_batch_quality(self, results: List[Dict], ground_truth: List[Dict] = None) -> Dict:
        """批量质量评估"""
        if ground_truth:
            return self._evaluate_with_ground_truth(results, ground_truth)
        else:
            return self._evaluate_internal_consistency(results)
    
    def _evaluate_internal_consistency(self, results: List[Dict]) -> Dict:
        """内部一致性评估"""
        consistency_scores = []
        confidence_scores = []
        
        for result in results:
            # 检查标签层级一致性
            labels = result.get("labels", [])
            hierarchy_consistent = self._check_hierarchy_consistency(labels)
            
            # 检查置信度合理性
            confidences = [label.get("confidence", 0) for label in labels]
            confidence_reasonable = all(0 <= conf <= 1 for conf in confidences)
            
            # 检查推理步骤连贯性
            reasoning_trace = result.get("reasoning_trace", [])
            reasoning_coherent = self._check_reasoning_coherence(reasoning_trace)
            
            consistency_score = (hierarchy_consistent + confidence_reasonable + reasoning_coherent) / 3
            consistency_scores.append(consistency_score)
            
            overall_confidence = result.get("overall_analysis", {}).get("confidence", 0)
            confidence_scores.append(overall_confidence)
        
        return {
            "average_consistency": sum(consistency_scores) / len(consistency_scores),
            "average_confidence": sum(confidence_scores) / len(confidence_scores),
            "high_quality_ratio": len([s for s in consistency_scores if s > 0.8]) / len(consistency_scores)
        }
```

### 6.2 实时监控系统
```python
class RealTimeMonitor:
    def __init__(self):
        self.metrics = {
            "api_calls_per_minute": 0,
            "average_response_time": 0.0,
            "error_rate": 0.0,
            "confidence_trend": []
        }
    
    def log_request(self, processing_time: float, confidence: float, has_error: bool):
        """记录请求指标"""
        # 更新处理时间
        self.metrics["average_response_time"] = (
            self.metrics["average_response_time"] * 0.9 + processing_time * 0.1
        )
        
        # 更新置信度趋势
        self.metrics["confidence_trend"].append(confidence)
        if len(self.metrics["confidence_trend"]) > 100:
            self.metrics["confidence_trend"].pop(0)
        
        # 更新错误率
        if has_error:
            self.metrics["error_rate"] = self.metrics["error_rate"] * 0.95 + 0.05
        else:
            self.metrics["error_rate"] = self.metrics["error_rate"] * 0.95
    
    def get_alert_conditions(self) -> List[str]:
        """检查告警条件"""
        alerts = []
        
        if self.metrics["average_response_time"] > 30:
            alerts.append("响应时间过长")
        
        if self.metrics["error_rate"] > 0.1:
            alerts.append("错误率过高")
        
        if len(self.metrics["confidence_trend"]) > 10:
            recent_confidence = sum(self.metrics["confidence_trend"][-10:]) / 10
            if recent_confidence < 0.5:
                alerts.append("置信度下降")
        
        return alerts
```

## 7. 部署与运维

### 7.1 Docker容器化部署
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .
RUN pip install -r requirements.txt

# 复制应用代码
COPY . .

# 下载中文字体（用于词云图）
RUN wget -O /app/fonts/simhei.ttf https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf || true

# 设置环境变量
ENV PYTHONPATH=/app
ENV FLASK_APP=app.py

EXPOSE 5000

# 启动命令
CMD ["python", "app.py"]
```

### 7.2 Kubernetes部署配置
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auto-label-analyzer
spec:
  replicas: 3
  selector:
    matchLabels:
      app: auto-label-analyzer
  template:
    metadata:
      labels:
        app: auto-label-analyzer
    spec:
      containers:
      - name: analyzer
        image: auto-label-analyzer:latest
        ports:
        - containerPort: 5000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
---
apiVersion: v1
kind: Service
metadata:
  name: analyzer-service
spec:
  selector:
    app: auto-label-analyzer
  ports:
  - port: 80
    targetPort: 5000
  type: LoadBalancer
```

## 总结

本增强版方案通过引入向量技术和多步推理策略，显著提升了汽车工单标签标记与情感分析的智能化水平：

### 核心优势
1. **向量化智能检索**：基于语义相似度的精准标签匹配
2. **多步推理策略**：从粗粒度到细粒度的渐进式分析
3. **自适应处理**：根据置信度动态调整分析策略
4. **全面可视化**：多维度图表展示分析结果
5. **企业级部署**：支持大规模并发和容器化部署

### 技术创新
- **混合检索**：TF-IDF + 语义向量 + 大模型推理
- **置信度递进**：多轮推理逐步提升分析质量
- **智能重试**：对低置信度结果自动优化
- **实时监控**：全面的性能和质量监控体系

该方案能够高效处理复杂的汽车行业标签体系，为企业提供准确的用户反馈分析和决策支持。 