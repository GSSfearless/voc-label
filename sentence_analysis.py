#!/usr/bin/env python3
"""
å¥å­åˆ‡åˆ†ç»“æœåˆ†æè„šæœ¬
ç”¨äºåˆ†æé¢„å¤„ç†åçš„å¥å­åˆ‡åˆ†ç»“æœï¼Œå¹¶å¯¼å‡ºä¸ºç‹¬ç«‹çš„å¥å­è¡¨
"""

import pandas as pd
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SentenceAnalyzer:
    """å¥å­åˆ‡åˆ†ç»“æœåˆ†æå™¨"""
    
    def __init__(self, input_file: str):
        self.input_file = input_file
        self.df = None
    
    def load_data(self) -> pd.DataFrame:
        """åŠ è½½é¢„å¤„ç†ç»“æœæ–‡ä»¶"""
        try:
            self.df = pd.read_csv(self.input_file)
            logger.info(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(self.df)} è¡Œ")
            return self.df
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            raise
    
    def extract_sentences(self) -> pd.DataFrame:
        """æå–æ‰€æœ‰å¥å­ï¼Œç”Ÿæˆå¥å­è¡¨"""
        if self.df is None:
            self.load_data()
        
        sentences_data = []
        
        for idx, row in self.df.iterrows():
            if not row.get('processing_success', False):
                continue
            
            # è·å–åŸå§‹æ•°æ®
            original_id = row.get('åºå·', idx)  # ä½¿ç”¨åºå·æˆ–ç´¢å¼•ä½œä¸ºåŸå§‹ID
            original_text = row.get('æ­£æ–‡', '')
            cleaned_text = row.get('cleaned_text', '')
            
            # è§£æå¥å­è¯¦æƒ…
            sentences_detail = row.get('sentences_detail', '')
            if not sentences_detail:
                continue
            
            try:
                sentences = json.loads(sentences_detail)
                
                for i, sentence in enumerate(sentences):
                    sentences_data.append({
                        'original_id': original_id,
                        'sentence_index': i,
                        'sentence_text': sentence.get('text', ''),
                        'sentence_start': sentence.get('start', 0),
                        'sentence_end': sentence.get('end', 0),
                        'sentence_lang': sentence.get('lang', ''),
                        'original_text': original_text,
                        'cleaned_text': cleaned_text,
                        'original_length': len(original_text),
                        'sentence_length': len(sentence.get('text', '')),
                    })
            except json.JSONDecodeError as e:
                logger.warning(f"è§£æå¥å­è¯¦æƒ…å¤±è´¥ (è¡Œ {idx}): {e}")
                continue
        
        sentences_df = pd.DataFrame(sentences_data)
        logger.info(f"æå–åˆ° {len(sentences_df)} ä¸ªå¥å­")
        return sentences_df
    
    def analyze_sentences(self, sentences_df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå¥å­ç»Ÿè®¡ä¿¡æ¯"""
        analysis = {}
        
        # åŸºæœ¬ç»Ÿè®¡
        analysis['total_sentences'] = len(sentences_df)
        analysis['total_original_texts'] = sentences_df['original_id'].nunique()
        analysis['avg_sentences_per_text'] = len(sentences_df) / analysis['total_original_texts']
        
        # å¥å­é•¿åº¦ç»Ÿè®¡
        analysis['sentence_length_stats'] = {
            'mean': sentences_df['sentence_length'].mean(),
            'median': sentences_df['sentence_length'].median(),
            'min': sentences_df['sentence_length'].min(),
            'max': sentences_df['sentence_length'].max(),
            'std': sentences_df['sentence_length'].std()
        }
        
        # è¯­è¨€åˆ†å¸ƒ
        if 'sentence_lang' in sentences_df.columns:
            lang_dist = sentences_df['sentence_lang'].value_counts().to_dict()
            analysis['language_distribution'] = lang_dist
        
        # å¥å­æ•°é‡åˆ†å¸ƒ
        sentences_per_text = sentences_df.groupby('original_id').size()
        analysis['sentences_per_text_stats'] = {
            'mean': sentences_per_text.mean(),
            'median': sentences_per_text.median(),
            'min': sentences_per_text.min(),
            'max': sentences_per_text.max(),
            'std': sentences_per_text.std()
        }
        
        return analysis
    
    def export_sentences(self, output_file: str, sentences_df: pd.DataFrame = None):
        """å¯¼å‡ºå¥å­è¡¨"""
        if sentences_df is None:
            sentences_df = self.extract_sentences()
        
        try:
            sentences_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            logger.info(f"å¥å­è¡¨å·²å¯¼å‡ºåˆ°: {output_file}")
        except Exception as e:
            logger.error(f"å¯¼å‡ºå¥å­è¡¨å¤±è´¥: {e}")
            raise
    
    def export_summary(self, output_file: str, analysis: Dict[str, Any]):
        """å¯¼å‡ºåˆ†ææ‘˜è¦"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("# å¥å­åˆ‡åˆ†åˆ†ææŠ¥å‘Š\n\n")
                
                # åŸºæœ¬ç»Ÿè®¡
                f.write("## åŸºæœ¬ç»Ÿè®¡\n")
                f.write(f"- æ€»å¥å­æ•°: {analysis['total_sentences']}\n")
                f.write(f"- åŸå§‹æ–‡æœ¬æ•°: {analysis['total_original_texts']}\n")
                f.write(f"- å¹³å‡æ¯æ¡æ–‡æœ¬å¥æ•°: {analysis['avg_sentences_per_text']:.2f}\n\n")
                
                # å¥å­é•¿åº¦ç»Ÿè®¡
                f.write("## å¥å­é•¿åº¦ç»Ÿè®¡\n")
                length_stats = analysis['sentence_length_stats']
                f.write(f"- å¹³å‡é•¿åº¦: {length_stats['mean']:.1f} å­—ç¬¦\n")
                f.write(f"- ä¸­ä½æ•°é•¿åº¦: {length_stats['median']:.1f} å­—ç¬¦\n")
                f.write(f"- æœ€çŸ­å¥å­: {length_stats['min']} å­—ç¬¦\n")
                f.write(f"- æœ€é•¿å¥å­: {length_stats['max']} å­—ç¬¦\n")
                f.write(f"- æ ‡å‡†å·®: {length_stats['std']:.1f}\n\n")
                
                # æ¯æ¡æ–‡æœ¬çš„å¥å­æ•°é‡ç»Ÿè®¡
                f.write("## æ¯æ¡æ–‡æœ¬å¥å­æ•°é‡ç»Ÿè®¡\n")
                sent_stats = analysis['sentences_per_text_stats']
                f.write(f"- å¹³å‡å¥å­æ•°: {sent_stats['mean']:.1f}\n")
                f.write(f"- ä¸­ä½æ•°å¥å­æ•°: {sent_stats['median']:.1f}\n")
                f.write(f"- æœ€å°‘å¥å­æ•°: {sent_stats['min']}\n")
                f.write(f"- æœ€å¤šå¥å­æ•°: {sent_stats['max']}\n")
                f.write(f"- æ ‡å‡†å·®: {sent_stats['std']:.1f}\n\n")
                
                # è¯­è¨€åˆ†å¸ƒ
                if 'language_distribution' in analysis:
                    f.write("## è¯­è¨€åˆ†å¸ƒ\n")
                    for lang, count in analysis['language_distribution'].items():
                        f.write(f"- {lang}: {count} å¥å­\n")
                    f.write("\n")
            
            logger.info(f"åˆ†ææ‘˜è¦å·²å¯¼å‡ºåˆ°: {output_file}")
        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†ææ‘˜è¦å¤±è´¥: {e}")
            raise
    
    def generate_sample_sentences(self, sentences_df: pd.DataFrame, num_samples: int = 10) -> pd.DataFrame:
        """ç”Ÿæˆå¥å­æ ·æœ¬"""
        # æŒ‰é•¿åº¦åˆ†å±‚é‡‡æ ·
        length_bins = pd.qcut(sentences_df['sentence_length'], q=4, labels=['çŸ­', 'ä¸­çŸ­', 'ä¸­é•¿', 'é•¿'])
        samples = []
        
        for bin_name in length_bins.cat.categories:
            bin_data = sentences_df[length_bins == bin_name]
            if len(bin_data) > 0:
                sample_size = min(num_samples // 4, len(bin_data))
                samples.append(bin_data.sample(n=sample_size))
        
        sample_df = pd.concat(samples, ignore_index=True) if samples else pd.DataFrame()
        return sample_df

def main():
    parser = argparse.ArgumentParser(description='å¥å­åˆ‡åˆ†ç»“æœåˆ†æ')
    parser.add_argument('input_file', help='é¢„å¤„ç†ç»“æœCSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-sentences', '-s', default='sentences.csv', help='å¥å­è¡¨è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--output-summary', '-r', default='sentence_analysis_report.md', help='åˆ†ææŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--output-samples', default='sentence_samples.csv', help='å¥å­æ ·æœ¬è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--sample-size', type=int, default=20, help='æ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    if not Path(args.input_file).exists():
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = SentenceAnalyzer(args.input_file)
        
        # æå–å¥å­
        logger.info("æ­£åœ¨æå–å¥å­...")
        sentences_df = analyzer.extract_sentences()
        
        if len(sentences_df) == 0:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¥å­åˆ‡åˆ†ç»“æœ")
            return
        
        # åˆ†æå¥å­
        logger.info("æ­£åœ¨åˆ†æå¥å­ç»Ÿè®¡ä¿¡æ¯...")
        analysis = analyzer.analyze_sentences(sentences_df)
        
        # å¯¼å‡ºç»“æœ
        logger.info("æ­£åœ¨å¯¼å‡ºç»“æœ...")
        analyzer.export_sentences(args.output_sentences, sentences_df)
        analyzer.export_summary(args.output_summary, analysis)
        
        # ç”Ÿæˆæ ·æœ¬
        sample_df = analyzer.generate_sample_sentences(sentences_df, args.sample_size)
        if len(sample_df) > 0:
            sample_df.to_csv(args.output_samples, index=False, encoding='utf-8-sig')
            logger.info(f"å¥å­æ ·æœ¬å·²å¯¼å‡ºåˆ°: {args.output_samples}")
        
        # æ˜¾ç¤ºæ‘˜è¦
        print("\n" + "="*50)
        print("ğŸ“Š å¥å­åˆ‡åˆ†åˆ†æå®Œæˆ")
        print(f"ğŸ“ æ€»å¥å­æ•°: {analysis['total_sentences']}")
        print(f"ğŸ“„ åŸå§‹æ–‡æœ¬æ•°: {analysis['total_original_texts']}")
        print(f"ğŸ“ å¹³å‡æ¯æ¡æ–‡æœ¬å¥æ•°: {analysis['avg_sentences_per_text']:.2f}")
        print(f"ğŸ“ å¥å­å¹³å‡é•¿åº¦: {analysis['sentence_length_stats']['mean']:.1f} å­—ç¬¦")
        print(f"ğŸ“„ ç»“æœå·²å¯¼å‡ºåˆ°:")
        print(f"   - å¥å­è¡¨: {args.output_sentences}")
        print(f"   - åˆ†ææŠ¥å‘Š: {args.output_summary}")
        print(f"   - å¥å­æ ·æœ¬: {args.output_samples}")
        
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    main() 