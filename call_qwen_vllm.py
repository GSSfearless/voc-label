#!/usr/bin/env python3
"""
è°ƒç”¨Qwen3:1.7B vLLMæœåŠ¡çš„è„šæœ¬
æœåŠ¡åœ°å€ï¼š36.103.199.82:8000

ä½¿ç”¨æ–¹æ³•ï¼š
1. å•æ¬¡è°ƒç”¨ï¼špython call_qwen_vllm.py --prompt "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
2. æ‰¹é‡å¤„ç†CSVæ–‡ä»¶ï¼špython call_qwen_vllm.py --batch --input data.csv --column text_column
3. äº¤äº’æ¨¡å¼ï¼špython call_qwen_vllm.py --interactive

é…ç½®è¯´æ˜ï¼š
- å¯ä»¥ä¿®æ”¹ VLLM_BASE_URL æ¥æ›´æ”¹æœåŠ¡åœ°å€
- å¯ä»¥è°ƒæ•´ max_tokensã€temperature ç­‰å‚æ•°
- æ”¯æŒç³»ç»Ÿæç¤ºè¯å’Œå¯¹è¯å†å²
"""

import aiohttp
import asyncio
import json
import pandas as pd
import argparse
import sys
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# vLLMæœåŠ¡é…ç½®
VLLM_BASE_URL = "http://36.103.199.82:8000"
VLLM_MODEL = "Qwen3:1.7B"  # æ¨¡å‹åç§°ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´

class QwenVLLMClient:
    """Qwen vLLMå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = VLLM_BASE_URL, model: str = VLLM_MODEL):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.session = None
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        timeout = aiohttp.ClientTimeout(total=120)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.session:
            await self.session.close()
    
    async def call_api(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
        top_p: float = 0.9,
        stop: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨vLLM API
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
            top_p: top-pé‡‡æ ·å‚æ•°
            stop: åœæ­¢è¯åˆ—è¡¨
            
        Returns:
            APIå“åº”ç»“æœ
        """
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "stream": False  # ä¸ä½¿ç”¨æµå¼è¾“å‡º
        }
        
        if stop:
            data["stop"] = stop
        
        try:
            async with self.session.post(
                f"{self.base_url}/v1/chat/completions",
                json=data,
                headers={"Content-Type": "application/json"}
            ) as response:
                
                if response.status == 200:
                    result = await response.json()
                    return {
                        "success": True,
                        "content": result["choices"][0]["message"]["content"],
                        "usage": result.get("usage", {}),
                        "response": result
                    }
                else:
                    error_text = await response.text()
                    logger.error(f"APIè°ƒç”¨å¤±è´¥: HTTP {response.status}, {error_text}")
                    return {
                        "success": False,
                        "error": f"HTTP {response.status}: {error_text}",
                        "content": ""
                    }
                    
        except asyncio.TimeoutError:
            logger.error("APIè°ƒç”¨è¶…æ—¶")
            return {
                "success": False,
                "error": "è¯·æ±‚è¶…æ—¶",
                "content": ""
            }
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": str(e),
                "content": ""
            }
    
    async def batch_process(
        self,
        texts: List[str],
        system_prompt: Optional[str] = None,
        max_concurrent: int = 5,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æ–‡æœ¬åˆ—è¡¨
        
        Args:
            texts: è¦å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            **kwargs: å…¶ä»–APIå‚æ•°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_one(text: str, index: int):
            async with semaphore:
                logger.info(f"å¤„ç†ç¬¬ {index + 1}/{len(texts)} æ¡æ–‡æœ¬")
                result = await self.call_api(text, system_prompt, **kwargs)
                result["index"] = index
                result["input_text"] = text
                return result
        
        tasks = [process_one(text, i) for i, text in enumerate(texts)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"å¤„ç†ç¬¬ {i + 1} æ¡æ–‡æœ¬æ—¶å‘ç”Ÿå¼‚å¸¸: {result}")
                processed_results.append({
                    "success": False,
                    "error": str(result),
                    "content": "",
                    "index": i,
                    "input_text": texts[i] if i < len(texts) else ""
                })
            else:
                processed_results.append(result)
        
        return processed_results

async def single_call(prompt: str, system_prompt: Optional[str] = None):
    """å•æ¬¡è°ƒç”¨ç¤ºä¾‹"""
    async with QwenVLLMClient() as client:
        print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨Qwen3:1.7Bæ¨¡å‹...")
        print(f"ğŸ“ è¾“å…¥: {prompt}")
        print("-" * 50)
        
        result = await client.call_api(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=1024,
            temperature=0.7
        )
        
        if result["success"]:
            print(f"âœ… å“åº”: {result['content']}")
            if "usage" in result:
                usage = result["usage"]
                print(f"ğŸ“Š ä½¿ç”¨ç»Ÿè®¡: {usage}")
        else:
            print(f"âŒ è°ƒç”¨å¤±è´¥: {result['error']}")

async def batch_process_csv(
    input_file: str,
    column_name: str,
    output_file: Optional[str] = None,
    system_prompt: Optional[str] = None,
    max_concurrent: int = 5
):
    """æ‰¹é‡å¤„ç†CSVæ–‡ä»¶"""
    
    # è¯»å–CSVæ–‡ä»¶
    try:
        df = pd.read_csv(input_file)
        logger.info(f"æˆåŠŸè¯»å–CSVæ–‡ä»¶: {input_file}, å…± {len(df)} è¡Œ")
    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        return
    
    if column_name not in df.columns:
        print(f"âŒ åˆ— '{column_name}' ä¸å­˜åœ¨äºCSVæ–‡ä»¶ä¸­")
        print(f"å¯ç”¨åˆ—: {list(df.columns)}")
        return
    
    # è¿‡æ»¤æ‰ç©ºå€¼
    valid_mask = df[column_name].notna() & (df[column_name] != "")
    texts = df[valid_mask][column_name].tolist()
    
    if not texts:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
        return
    
    print(f"ğŸ“Š å‡†å¤‡å¤„ç† {len(texts)} æ¡æ–‡æœ¬")
    print(f"ğŸš€ æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
    
    # æ‰¹é‡å¤„ç†
    async with QwenVLLMClient() as client:
        start_time = time.time()
        results = await client.batch_process(
            texts=texts,
            system_prompt=system_prompt,
            max_concurrent=max_concurrent
        )
        end_time = time.time()
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r["success"])
        failure_count = len(results) - success_count
        
        print(f"âœ… å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸ: {success_count}, å¤±è´¥: {failure_count}")
        print(f"â±ï¸ è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # å°†ç»“æœæ·»åŠ åˆ°DataFrame
        df["qwen_response"] = ""
        df["qwen_success"] = False
        df["qwen_error"] = ""
        
        for result in results:
            idx = result["index"]
            original_idx = df[valid_mask].index[idx]
            df.loc[original_idx, "qwen_response"] = result["content"]
            df.loc[original_idx, "qwen_success"] = result["success"]
            if not result["success"]:
                df.loc[original_idx, "qwen_error"] = result["error"]
        
        # ä¿å­˜ç»“æœ
        if output_file is None:
            input_path = Path(input_file)
            output_file = input_path.stem + "_qwen_results" + input_path.suffix
        
        df.to_csv(output_file, index=False)
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœé¢„è§ˆ
        print("\nğŸ“‹ ç»“æœé¢„è§ˆ:")
        for i, result in enumerate(results[:3]):
            if result["success"]:
                print(f"  {i+1}. è¾“å…¥: {result['input_text'][:50]}...")
                print(f"     è¾“å‡º: {result['content'][:100]}...")
            else:
                print(f"  {i+1}. é”™è¯¯: {result['error']}")

async def interactive_mode():
    """äº¤äº’æ¨¡å¼"""
    print("ğŸ¤– è¿›å…¥Qwen3:1.7Bäº¤äº’æ¨¡å¼")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'system:ä½ çš„ç³»ç»Ÿæç¤ºè¯' è®¾ç½®ç³»ç»Ÿæç¤ºè¯")
    print("-" * 50)
    
    system_prompt = None
    
    async with QwenVLLMClient() as client:
        while True:
            try:
                user_input = input("\nğŸ’¬ æ‚¨: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if user_input.startswith('system:'):
                    system_prompt = user_input[7:].strip()
                    print(f"âœ… ç³»ç»Ÿæç¤ºè¯å·²è®¾ç½®: {system_prompt}")
                    continue
                
                if not user_input:
                    continue
                
                print("ğŸ¤– Qwen: ", end="", flush=True)
                result = await client.call_api(
                    prompt=user_input,
                    system_prompt=system_prompt,
                    max_tokens=1024,
                    temperature=0.7
                )
                
                if result["success"]:
                    print(result["content"])
                else:
                    print(f"âŒ é”™è¯¯: {result['error']}")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿå¼‚å¸¸: {e}")

async def test_connection():
    """æµ‹è¯•è¿æ¥"""
    print("ğŸ” æµ‹è¯•vLLMæœåŠ¡è¿æ¥...")
    
    async with QwenVLLMClient() as client:
        result = await client.call_api(
            prompt="ä½ å¥½",
            max_tokens=50
        )
        
        if result["success"]:
            print("âœ… è¿æ¥æˆåŠŸï¼")
            print(f"ğŸ“¤ æµ‹è¯•è¾“å…¥: ä½ å¥½")
            print(f"ğŸ“¥ æµ‹è¯•è¾“å‡º: {result['content']}")
            return True
        else:
            print(f"âŒ è¿æ¥å¤±è´¥: {result['error']}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è°ƒç”¨Qwen3:1.7B vLLMæœåŠ¡")
    parser.add_argument("--prompt", type=str, help="å•æ¬¡è°ƒç”¨çš„æç¤ºè¯")
    parser.add_argument("--system", type=str, help="ç³»ç»Ÿæç¤ºè¯")
    parser.add_argument("--batch", action="store_true", help="æ‰¹é‡å¤„ç†æ¨¡å¼")
    parser.add_argument("--input", type=str, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--column", type=str, help="è¦å¤„ç†çš„åˆ—å")
    parser.add_argument("--output", type=str, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--concurrent", type=int, default=5, help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--interactive", action="store_true", help="äº¤äº’æ¨¡å¼")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•è¿æ¥")
    
    args = parser.parse_args()
    
    # æµ‹è¯•è¿æ¥
    if args.test:
        asyncio.run(test_connection())
        return
    
    # äº¤äº’æ¨¡å¼
    if args.interactive:
        asyncio.run(interactive_mode())
        return
    
    # æ‰¹é‡å¤„ç†æ¨¡å¼
    if args.batch:
        if not args.input or not args.column:
            print("âŒ æ‰¹é‡å¤„ç†æ¨¡å¼éœ€è¦æŒ‡å®š --input å’Œ --column å‚æ•°")
            return
        
        asyncio.run(batch_process_csv(
            input_file=args.input,
            column_name=args.column,
            output_file=args.output,
            system_prompt=args.system,
            max_concurrent=args.concurrent
        ))
        return
    
    # å•æ¬¡è°ƒç”¨æ¨¡å¼
    if args.prompt:
        asyncio.run(single_call(args.prompt, args.system))
        return
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå‚æ•°ï¼Œæ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print("ğŸ¤– Qwen3:1.7B vLLMè°ƒç”¨è„šæœ¬")
    print("=" * 50)
    print("ä½¿ç”¨ç¤ºä¾‹:")
    print("1. æµ‹è¯•è¿æ¥:")
    print("   python call_qwen_vllm.py --test")
    print("\n2. å•æ¬¡è°ƒç”¨:")
    print("   python call_qwen_vllm.py --prompt 'è¯·ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—'")
    print("\n3. å¸¦ç³»ç»Ÿæç¤ºè¯çš„è°ƒç”¨:")
    print("   python call_qwen_vllm.py --prompt 'åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿ' --system 'ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä¸“å®¶'")
    print("\n4. æ‰¹é‡å¤„ç†CSV:")
    print("   python call_qwen_vllm.py --batch --input data.csv --column text_column")
    print("\n5. äº¤äº’æ¨¡å¼:")
    print("   python call_qwen_vllm.py --interactive")
    print("\n6. æŸ¥çœ‹å¸®åŠ©:")
    print("   python call_qwen_vllm.py --help")

if __name__ == "__main__":
    main() 