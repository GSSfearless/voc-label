"""
æ•°æ®æ¸…æ´—æ¨¡å—
è´Ÿè´£å¯¹åŸå§‹æ•°æ®è¿›è¡Œæ¸…æ´—ã€è¿‡æ»¤å’Œæ ‡å‡†åŒ–
"""

import pandas as pd
import numpy as np
import re
import logging
from typing import Dict, List, Optional, Union, Callable

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def clean_text(text: str) -> str:
    """
    æ¸…æ´—æ–‡æœ¬å†…å®¹
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        æ¸…æ´—åçš„æ–‡æœ¬
    """
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    # åˆ é™¤URL
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    
    # åˆ é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<.*?>', '', text)
    
    # åˆ é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šç¬¦å·
    text = re.sub(r'\[.*?\]', '', text)  # åˆ é™¤[è¡¨æƒ…]
    text = re.sub(r'#.*?#', '', text)    # åˆ é™¤å¾®åšè¯é¢˜æ ‡ç­¾
    
    # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
    text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼šï¼›""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€]+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å· (æ³¨é‡Šæ‰è¿™è¡Œä»¥ä¿ç•™ä¸­æ–‡å­—ç¬¦)
    # text = text.encode('ascii', 'ignore').decode('ascii')
    
    # ä¿®æ­£å¸¸è§é”™è¯¯
    text = re.sub(r'(https?)', '', text)
    
    return text.strip()

def filter_relevant_content(df: pd.DataFrame, keywords: List[str]) -> pd.DataFrame:
    """
    è¿‡æ»¤ä¸å…³é”®è¯ç›¸å…³çš„å†…å®¹
    
    Args:
        df: åŸå§‹æ•°æ®DataFrame
        keywords: å…³é”®è¯åˆ—è¡¨
        
    Returns:
        è¿‡æ»¤åçš„DataFrame
    """
    if 'text_content' not in df.columns:
        logger.error("æ•°æ®ä¸­ç¼ºå°‘text_contentåˆ—")
        return df
    
    # ç¡®ä¿text_contentåˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
    df['text_content'] = df['text_content'].astype(str)
    
    # æ„å»ºå…³é”®è¯æ­£åˆ™è¡¨è¾¾å¼
    pattern = '|'.join(keywords)
    
    # è¿‡æ»¤åŒ…å«å…³é”®è¯çš„è¡Œ
    mask = df['text_content'].str.contains(pattern, case=False, na=False)
    filtered_df = df[mask].copy()
    
    logger.info(f"è¿‡æ»¤å‰è¡Œæ•°: {len(df)}, è¿‡æ»¤åè¡Œæ•°: {len(filtered_df)}")
    return filtered_df

def filter_meaningful_content(df: pd.DataFrame, min_length: int = 5, min_chinese_chars: int = 2) -> pd.DataFrame:
    """
    è¿‡æ»¤æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œå»é™¤æ— æ„ä¹‰çš„çŸ­è¯„è®º
    
    Args:
        df: åŸå§‹æ•°æ®DataFrame
        min_length: æœ€å°æ–‡æœ¬é•¿åº¦
        min_chinese_chars: æœ€å°ä¸­æ–‡å­—ç¬¦æ•°
        
    Returns:
        è¿‡æ»¤åçš„DataFrame
    """
    if 'text_content' not in df.columns:
        logger.error("æ•°æ®ä¸­ç¼ºå°‘text_contentåˆ—")
        return df
    
    # ç¡®ä¿text_contentåˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
    df['text_content'] = df['text_content'].astype(str)
    
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°
    def count_chinese_chars(text):
        if not isinstance(text, str):
            return 0
        return len(re.findall(r'[\u4e00-\u9fff]', text))
    
    # åº”ç”¨è¿‡æ»¤æ¡ä»¶
    df['text_length'] = df['text_content'].str.len()
    df['chinese_chars'] = df['text_content'].apply(count_chinese_chars)
    
    # è¿‡æ»¤æ‰è¿‡çŸ­æˆ–æ²¡æœ‰è¶³å¤Ÿä¸­æ–‡å­—ç¬¦çš„è¯„è®º
    filtered_df = df[(df['text_length'] >= min_length) & (df['chinese_chars'] >= min_chinese_chars)].copy()
    
    # è¿‡æ»¤æ‰é€šç”¨æ— æ„ä¹‰è¯„è®º
    noise_patterns = [
        r'^[å¥½èµæ£’çœŸä¸é”™å—¯å“¦æ˜¯]+$',  # å•çº¯çš„å¥½ã€èµã€æ£’ç­‰
        r'^[ï¼Ÿã€‚ï¼Œï¼]+$',           # åªæœ‰æ ‡ç‚¹ç¬¦å·
        r'^[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿]+$',  # åªæœ‰æ•°å­—
        r'^(666+|åƒåœ¾|å‘µå‘µ|å“ˆå“ˆ+|å‰å®³|å¯ä»¥|nice|good|ok|ğŸ˜Š|ã€‚ã€‚ã€‚)$',  # å¸¸è§æ— æ„ä¹‰è¯„è®º
        r'^([.ã€‚]{2,}|[?ï¼Ÿ]{2,}|[!ï¼]{2,})$',  # é‡å¤æ ‡ç‚¹
        r'å¿«æ¥æŠ¢è´­',   # ä¿ƒé”€ç±»åƒåœ¾è¯„è®º
        r'çº¯æ”¯æŒ',     # çº¯æ”¯æŒç±»æ— å®é™…å†…å®¹
        r'çº¯å…ƒæ°”',     # æ— æ„ä¹‰äº’åŠ¨ç±»
        r'^ä¸‹å•',      # ä»…è¡¨ç¤ºä¸‹å•
        r'^å·²è´­',      # ä»…è¡¨ç¤ºå·²è´­ä¹°
        r'^å‰æ¥',      # å‰æ¥æ‰“å¡ç±»
        r'^(å®‰è£…å¸ˆå‚…|å¸ˆå‚…|ç‰©æµ)',   # ä»…æåŠå®‰è£…å¸ˆå‚…æˆ–ç‰©æµ
        r'^(æ”¶åˆ°|åˆ°è´§)',  # ä»…è¡¨ç¤ºæ”¶åˆ°è´§
        r'^[\s\t\r\n]*$'  # ç©ºç™½è¯„è®º
    ]
    
    for pattern in noise_patterns:
        filtered_df = filtered_df[~filtered_df['text_content'].str.contains(pattern, regex=True, na=False)]
    
    # ç§»é™¤è¾…åŠ©åˆ—
    filtered_df.drop(columns=['text_length', 'chinese_chars'], inplace=True)
    
    logger.info(f"è¿‡æ»¤å‰è¡Œæ•°: {len(df)}, è¿‡æ»¤æ‰æ— æ„ä¹‰å†…å®¹åè¡Œæ•°: {len(filtered_df)}")
    return filtered_df

def remove_duplicates(df: pd.DataFrame, subset: Optional[List[str]] = None) -> pd.DataFrame:
    """
    ç§»é™¤é‡å¤æ•°æ®
    
    Args:
        df: åŸå§‹æ•°æ®DataFrame
        subset: ç”¨äºåˆ¤æ–­é‡å¤çš„åˆ—ï¼Œé»˜è®¤ä¸º['text_content']
        
    Returns:
        å»é‡åçš„DataFrame
    """
    if subset is None:
        subset = ['text_content']
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    for col in subset:
        if col not in df.columns:
            logger.warning(f"åˆ— {col} ä¸å­˜åœ¨ï¼Œå°†ä»å»é‡å­é›†ä¸­ç§»é™¤")
            subset.remove(col)
    
    if not subset:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„åˆ—ç”¨äºå»é‡")
        return df
    
    # å»é‡
    before_count = len(df)
    df_dedup = df.drop_duplicates(subset=subset, keep='first')
    after_count = len(df_dedup)
    
    logger.info(f"ç§»é™¤äº† {before_count - after_count} æ¡é‡å¤æ•°æ®")
    return df_dedup

def filter_by_date_range(df: pd.DataFrame, 
                        start_date: Optional[str] = None, 
                        end_date: Optional[str] = None, 
                        date_col: str = 'timestamp') -> pd.DataFrame:
    """
    æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤æ•°æ®
    
    Args:
        df: æ•°æ®DataFrame
        start_date: èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
        date_col: æ—¥æœŸåˆ—åï¼Œé»˜è®¤ä¸º'timestamp'
        
    Returns:
        è¿‡æ»¤åçš„DataFrame
    """
    if date_col not in df.columns:
        logger.error(f"åˆ— {date_col} ä¸å­˜åœ¨")
        return df
    
    # å°è¯•è½¬æ¢æ—¥æœŸåˆ—
    try:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    except Exception as e:
        logger.error(f"è½¬æ¢æ—¥æœŸåˆ—å¤±è´¥: {e}")
        return df
    
    # åº”ç”¨æ—¥æœŸè¿‡æ»¤
    if start_date is not None:
        try:
            start = pd.to_datetime(start_date)
            df = df[df[date_col] >= start]
            logger.info(f"è¿‡æ»¤å‡º {start_date} ä¹‹åçš„æ•°æ®")
        except Exception as e:
            logger.error(f"è½¬æ¢èµ·å§‹æ—¥æœŸå¤±è´¥: {e}")
    
    if end_date is not None:
        try:
            end = pd.to_datetime(end_date)
            df = df[df[date_col] <= end]
            logger.info(f"è¿‡æ»¤å‡º {end_date} ä¹‹å‰çš„æ•°æ®")
        except Exception as e:
            logger.error(f"è½¬æ¢ç»“æŸæ—¥æœŸå¤±è´¥: {e}")
    
    return df

def standardize_product_models(df: pd.DataFrame, model_col: str = 'product_model') -> pd.DataFrame:
    """
    æ ‡å‡†åŒ–äº§å“å‹å·åç§°
    
    Args:
        df: æ•°æ®DataFrame
        model_col: äº§å“å‹å·åˆ—å
        
    Returns:
        æ ‡å‡†åŒ–åçš„DataFrame
    """
    if model_col not in df.columns:
        logger.warning(f"åˆ— {model_col} ä¸å­˜åœ¨ï¼Œæ— æ³•æ ‡å‡†åŒ–äº§å“å‹å·")
        return df
    
    # ç¡®ä¿äº§å“å‹å·åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
    df[model_col] = df[model_col].astype(str)
    
    # å®šä¹‰å‹å·æ›¿æ¢è§„åˆ™ (æ­£åˆ™è¡¨è¾¾å¼ -> æ ‡å‡†å‹å·)
    model_mapping = {
        r'(?i)a2z': 'A2Z',
        r'(?i)a1z': 'A1Z',
        r'(?i)c25': 'C25',
        r'(?i)c40': 'C40',
        r'(?i)c65': 'C65',
        r'(?i)c80': 'C80',
        r'(?i)m95c': 'M95C',
        r'(?i)m65': 'M65',
        r'(?i)n[0-9]+c': lambda x: x.upper(),  # å°†NxxCæ ¼å¼è½¬ä¸ºå¤§å†™
        r'(?i)q[0-9]+': lambda x: x.upper(),   # å°†Qxxæ ¼å¼è½¬ä¸ºå¤§å†™
    }
    
    # åº”ç”¨æ›¿æ¢è§„åˆ™
    original_models = df[model_col].copy()
    
    for pattern, replacement in model_mapping.items():
        if callable(replacement):
            # å¦‚æœreplacementæ˜¯å‡½æ•°ï¼Œç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åŒ¹é…çš„å­ä¸²å¹¶åº”ç”¨å‡½æ•°
            mask = df[model_col].str.contains(pattern, na=False)
            matches = df.loc[mask, model_col].str.extract(f'({pattern})', expand=False)
            df.loc[mask, model_col] = matches.apply(replacement)
        else:
            # å¦åˆ™ç›´æ¥æ›¿æ¢
            df[model_col] = df[model_col].str.replace(pattern, replacement, regex=True)
    
    # ç»Ÿè®¡æ›¿æ¢æƒ…å†µ
    changed_count = (df[model_col] != original_models).sum()
    logger.info(f"å·²æ ‡å‡†åŒ– {changed_count} ä¸ªäº§å“å‹å·")
    
    return df

def validate_ecommerce_comments(df: pd.DataFrame, min_length: int = 5) -> pd.DataFrame:
    """
    éªŒè¯ç”µå•†è¯„è®ºæ•°æ®ï¼Œæ·»åŠ is_validæ ‡å¿—å­—æ®µï¼Œç”¨äºæ ‡è®°æ˜¯å¦éœ€è¦å¤§æ¨¡å‹è¿›è¡Œä¸‹ä¸€æ­¥åˆ†æ
    
    Args:
        df: ç”µå•†è¯„è®ºæ•°æ®DataFrame
        min_length: è¯„è®ºå†…å®¹æœ€å°é•¿åº¦ï¼Œé»˜è®¤ä¸º5
        
    Returns:
        æ·»åŠ is_validå­—æ®µçš„DataFrame
    """
    if 'è¯„è®ºå†…å®¹' not in df.columns:
        logger.error("æ•°æ®ä¸­ç¼ºå°‘è¯„è®ºå†…å®¹åˆ—")
        return df
    
    # å¤åˆ¶DataFrameé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    df_validated = df.copy()
    
    # åˆå§‹åŒ–is_validå­—æ®µä¸ºTrue
    df_validated['is_valid'] = True
    
    # å®šä¹‰æ— æ•ˆè¯„è®ºçš„æ¡ä»¶
    # 1. ç©ºè¯„è®ºæˆ–é»˜è®¤æ–‡æœ¬
    empty_patterns = [
        r'æ­¤ç”¨æˆ·',  # åŒ¹é…æ‰€æœ‰åŒ…å«"æ­¤ç”¨æˆ·"çš„è¯„è®º
        r'^$',  # ç©ºå­—ç¬¦ä¸²
        r'^\s+$'  # åªåŒ…å«ç©ºç™½
    ]
    
    for pattern in empty_patterns:
        mask = df_validated['è¯„è®ºå†…å®¹'].str.contains(pattern, regex=True, na=True)
        df_validated.loc[mask, 'is_valid'] = False
    
    # 2. è¿‡çŸ­çš„è¯„è®º
    df_validated.loc[df_validated['è¯„è®ºå†…å®¹'].str.len() < min_length, 'is_valid'] = False
    
    # 3. æ— å®è´¨æ€§å†…å®¹çš„è¯„è®º
    generic_patterns = [
        r'^[å¥½èµæ£’çœŸä¸é”™å—¯å“¦æ˜¯]+$',  # å•çº¯çš„å¥½ã€èµã€æ£’ç­‰
        r'^[ï¼Ÿã€‚ï¼Œï¼]+$',           # åªæœ‰æ ‡ç‚¹ç¬¦å·
        r'^[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿]+$',  # åªæœ‰æ•°å­—
        r'^(666+|åƒåœ¾|å‘µå‘µ|å“ˆå“ˆ+|å‰å®³|å¯ä»¥|nice|good|ok|ğŸ˜Š|ã€‚ã€‚ã€‚)$',  # å¸¸è§æ— æ„ä¹‰è¯„è®º
        r'^([.ã€‚]{2,}|[?ï¼Ÿ]{2,}|[!ï¼]{2,})$',  # é‡å¤æ ‡ç‚¹
    ]
    
    for pattern in generic_patterns:
        mask = df_validated['è¯„è®ºå†…å®¹'].str.contains(pattern, regex=True, na=False)
        df_validated.loc[mask, 'is_valid'] = False
    
    # å¯¹äºNaNå€¼æ ‡è®°ä¸ºæ— æ•ˆ
    df_validated.loc[df_validated['è¯„è®ºå†…å®¹'].isna(), 'is_valid'] = False
    
    # ç»Ÿè®¡æœ‰æ•ˆå’Œæ— æ•ˆè¯„è®ºæ•°é‡
    valid_count = df_validated['is_valid'].sum()
    total_count = len(df_validated)
    invalid_count = total_count - valid_count
    
    logger.info(f"æ€»è¯„è®ºæ•°: {total_count}, æœ‰æ•ˆè¯„è®ºæ•°: {valid_count}, æ— æ•ˆè¯„è®ºæ•°: {invalid_count}")
    logger.info(f"æœ‰æ•ˆè¯„è®ºæ¯”ä¾‹: {valid_count/total_count:.2%}")
    
    return df_validated

def clean_dataframe(df: pd.DataFrame, 
                   keywords: Optional[List[str]] = None, 
                   start_date: Optional[str] = None, 
                   end_date: Optional[str] = None) -> pd.DataFrame:
    """
    å¯¹æ•°æ®æ¡†è¿›è¡Œå…¨é¢æ¸…æ´—
    
    Args:
        df: åŸå§‹æ•°æ®DataFrame
        keywords: å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ç›¸å…³å†…å®¹
        start_date: èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
        
    Returns:
        æ¸…æ´—åçš„DataFrame
    """
    logger.info(f"å¼€å§‹æ¸…æ´—æ•°æ®ï¼ŒåŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")
    
    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ['text_content']
    for col in required_cols:
        if col not in df.columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {col}")
            return df
    
    # 1. æ¸…æ´—æ–‡æœ¬å†…å®¹
    logger.info("æ¸…æ´—æ–‡æœ¬å†…å®¹...")
    df['text_content'] = df['text_content'].apply(clean_text)
    
    # 2. åˆ é™¤ç©ºæ–‡æœ¬
    df = df[df['text_content'].str.len() > 0].reset_index(drop=True)
    logger.info(f"åˆ é™¤ç©ºæ–‡æœ¬åçš„è¡Œæ•°: {len(df)}")
    
    # 3. è¿‡æ»¤æœ‰æ„ä¹‰çš„å†…å®¹
    logger.info("è¿‡æ»¤æ— æ„ä¹‰å†…å®¹...")
    df = filter_meaningful_content(df, min_length=5, min_chinese_chars=2)
    
    # 4. å…³é”®è¯è¿‡æ»¤ï¼ˆå¦‚æœæä¾›ï¼‰
    if keywords:
        logger.info(f"ä½¿ç”¨å…³é”®è¯è¿‡æ»¤: {keywords}")
        df = filter_relevant_content(df, keywords)
    
    # 5. æ—¥æœŸèŒƒå›´è¿‡æ»¤ï¼ˆå¦‚æœæä¾›ï¼‰
    if start_date or end_date:
        logger.info(f"ä½¿ç”¨æ—¥æœŸèŒƒå›´è¿‡æ»¤: {start_date} åˆ° {end_date}")
        df = filter_by_date_range(df, start_date, end_date)
    
    # 6. æ ‡å‡†åŒ–äº§å“å‹å·
    if 'product_model' in df.columns:
        logger.info("æ ‡å‡†åŒ–äº§å“å‹å·...")
        df = standardize_product_models(df)
    
    # 7. å»é‡
    df = remove_duplicates(df)
    
    logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆè¡Œæ•°: {len(df)}")
    return df.reset_index(drop=True)

def clean_social_media_data(
    df: pd.DataFrame,
    text_col: str = 'text_content',
    author_col: Optional[str] = 'author',
    author_blacklist: Optional[List[str]] = None,
    meaningless_patterns: Optional[List[str]] = None,
    min_text_length: int = 3,
    clean_weibo_tags: bool = False
) -> pd.DataFrame:
    """
    å¯¹æ¥è‡ªç¤¾äº¤åª’ä½“ç­‰å…¬åŸŸæ¸ é“çš„æ•°æ®è¿›è¡Œé€šç”¨æ¸…æ´—ã€‚
    æ­¤ç‰ˆæœ¬ä¸ä¼šåˆ é™¤è¡Œï¼Œè€Œæ˜¯æ·»åŠ  'is_valid' æ ‡è®°åˆ—ã€‚

    Args:
        df: åŒ…å«å¾…æ¸…æ´—æ•°æ®çš„ Pandas DataFrameã€‚
        text_col: åŒ…å«ä¸»è¦æ–‡æœ¬å†…å®¹çš„åˆ—åï¼Œé»˜è®¤ä¸º 'text_content'ã€‚
        author_col: åŒ…å«ä½œè€…ä¿¡æ¯çš„åˆ—åï¼Œé»˜è®¤ä¸º 'author'ã€‚å¦‚æœä¸º None æˆ–åˆ—ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡ä½œè€…è¿‡æ»¤ã€‚
        author_blacklist: ä½œè€…å…³é”®è¯é»‘åå•åˆ—è¡¨ã€‚å¦‚æœä½œè€…åˆ—å†…å®¹åŒ…å«ä»»ä¸€å…³é”®è¯ï¼Œè¯¥è¡Œå°†è¢«æ ‡è®°ä¸ºæ— æ•ˆ (is_valid=False)ã€‚
        meaningless_patterns: ç”¨äºåŒ¹é…æ— æ„ä¹‰å†…å®¹çš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨ã€‚å¦‚æœæ–‡æœ¬å†…å®¹åŒ¹é…ä»»ä¸€æ¨¡å¼ï¼Œè¯¥è¡Œå°†è¢«æ ‡è®°ä¸ºæ— æ•ˆ (is_valid=False)ã€‚
        min_text_length: æ¸…æ´—åæ–‡æœ¬å†…å®¹çš„æœ€å°é•¿åº¦ï¼Œä½äºæ­¤é•¿åº¦çš„è¡Œå°†è¢«æ ‡è®°ä¸ºæ— æ•ˆ (is_valid=False)ã€‚
        clean_weibo_tags: æ˜¯å¦æ¸…æ´—å¾®åšæ ‡ç­¾ï¼ˆ#æ ‡ç­¾å†…å®¹#ï¼‰ã€‚

    Returns:
        åŒ…å«åŸå§‹æ•°æ®ä»¥åŠ 'is_valid', 'invalidation_reason', 'cleaned_text' åˆ—çš„ Pandas DataFrameã€‚
    """
    logger.info(f"å¼€å§‹é€šç”¨ç¤¾äº¤åª’ä½“æ•°æ®æ¸…æ´—ï¼ˆæ ‡è®°æ¨¡å¼ï¼‰ï¼Œåˆå§‹è¡Œæ•°: {len(df)}")
    original_count = len(df)
    cleaned_df = df.copy()

    # --- åˆå§‹åŒ–æ–°åˆ— ---
    cleaned_df['is_valid'] = True  # é»˜è®¤ä¸ºæœ‰æ•ˆ
    cleaned_df['invalidation_reason'] = '' # é‡å‘½å removal_reason
    # åˆå§‹åŒ– cleaned_textï¼Œç¡®ä¿å¤„ç† NaN å’Œéå­—ç¬¦ä¸²ç±»å‹
    cleaned_df['cleaned_text'] = cleaned_df[text_col].fillna('').astype(str)


    # 0. æ£€æŸ¥æ–‡æœ¬åˆ—æ˜¯å¦å­˜åœ¨
    if text_col not in cleaned_df.columns:
        logger.error(f"é”™è¯¯ï¼šæŒ‡å®šçš„æ–‡æœ¬åˆ— '{text_col}' ä¸åœ¨ DataFrame ä¸­ã€‚æ¸…æ´—ä¸­æ­¢ã€‚")
        # å³ä½¿ä¸­æ­¢ï¼Œä¹Ÿè¿”å›å¸¦æœ‰åˆå§‹åŒ–åˆ—çš„ df
        cleaned_df['is_valid'] = False # æ ‡è®°æ‰€æœ‰è¡Œä¸ºæ— æ•ˆï¼Œå› ä¸ºæ— æ³•å¤„ç†
        cleaned_df['invalidation_reason'] = 'missing_text_column'
        return cleaned_df


    # 1. é€šè¿‡ä½œè€…å…³é”®å­—ç­›é€‰
    if author_col and author_col in cleaned_df.columns and author_blacklist:
        # ç¡®ä¿ä½œè€…åˆ—æ˜¯å­—ç¬¦ä¸²
        cleaned_df[author_col] = cleaned_df[author_col].fillna('').astype(str)
        pattern = '|'.join([re.escape(keyword) for keyword in author_blacklist])
        mask = cleaned_df[author_col].str.contains(pattern, case=False, na=False)

        # æ ‡è®°ä¸ºæ— æ•ˆ
        cleaned_df.loc[mask & cleaned_df['is_valid'], 'invalidation_reason'] = 'author_blacklist' # åªè®°å½•é¦–æ¬¡å¤±æ•ˆåŸå› 
        cleaned_df.loc[mask, 'is_valid'] = False
        marked_count = mask.sum()
        if marked_count > 0:
            logger.info(f"æ­¥éª¤ 1/6: æ ¹æ®ä½œè€…é»‘åå•æ ‡è®°äº† {marked_count} è¡Œä¸ºæ— æ•ˆã€‚")
    elif author_col and author_col not in cleaned_df.columns:
         logger.warning(f"è­¦å‘Šï¼šæŒ‡å®šçš„ä½œè€…åˆ— '{author_col}' ä¸åœ¨ DataFrame ä¸­ã€‚è·³è¿‡ä½œè€…è¿‡æ»¤ã€‚")
    elif not author_col:
        logger.info("æ­¥éª¤ 1/6: æœªæŒ‡å®šä½œè€…åˆ—ï¼Œè·³è¿‡ä½œè€…è¿‡æ»¤ã€‚")
    elif not author_blacklist:
         logger.info("æ­¥éª¤ 1/6: æœªæä¾›ä½œè€…é»‘åå•ï¼Œè·³è¿‡ä½œè€…è¿‡æ»¤ã€‚")


    # 2. æ¸…æ´—å¾®åšè½¬å‘ (ä¿®æ”¹ cleaned_text åˆ—)
    cleaned_df['cleaned_text'] = cleaned_df['cleaned_text'].apply(lambda x: x.split('//')[0].strip() if '//' in x else x)
    # æ£€æŸ¥æ˜¯å¦å› æˆªæ–­å¯¼è‡´ cleaned_text ä¸ºç©ºï¼Œå¹¶æ ‡è®°ä¸ºæ— æ•ˆ
    empty_after_split_mask = cleaned_df['cleaned_text'].str.strip() == ''
    cleaned_df.loc[empty_after_split_mask & cleaned_df['is_valid'], 'invalidation_reason'] = 'empty_after_forward_clean'
    cleaned_df.loc[empty_after_split_mask, 'is_valid'] = False
    marked_count = (empty_after_split_mask & ~mask).sum() if 'mask' in locals() else empty_after_split_mask.sum() # è®¡ç®—æœ¬æ¬¡æ–°å¢çš„æ— æ•ˆæ ‡è®°
    if marked_count > 0:
         logger.info(f"æ­¥éª¤ 2/6: æ¸…ç†å¾®åšè½¬å‘åï¼Œæ ‡è®°äº† {marked_count} è¡Œå› å†…å®¹ä¸ºç©ºè€Œæ— æ•ˆã€‚")
    else:
        logger.info("æ­¥éª¤ 2/6: å®Œæˆå¾®åšè½¬å‘æ¸…ç†ï¼ˆæˆ–æ— éœ€æ¸…ç†ï¼‰ã€‚")

    # 3. æ¸…æ´—å¾®åšæ ‡ç­¾ (å¦‚æœå¯ç”¨)
    if clean_weibo_tags:
        # æ”¹è¿›çš„å¤„ç†æ–¹å¼ï¼Œå¤„ç†ä»¥#å¼€å¤´åˆ°ç©ºæ ¼ç»“æŸçš„æ ‡ç­¾æ ¼å¼
        def clean_all_weibo_tags(text):
            if not isinstance(text, str):
                return ""
            # åŒ¹é…æ‰€æœ‰ä»¥#å¼€å¤´ã€åˆ°ç©ºæ ¼æˆ–å­—ç¬¦ä¸²ç»“å°¾çš„å†…å®¹
            cleaned_text = re.sub(r'#\S+\s*', '', text).strip()
            # å¤„ç†å¤šä½™çš„ç©ºæ ¼
            cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
            return cleaned_text
            
        cleaned_df['cleaned_text'] = cleaned_df['cleaned_text'].apply(clean_all_weibo_tags)
        
        # æ£€æŸ¥æ˜¯å¦å› æ¸…æ´—æ ‡ç­¾å¯¼è‡´å†…å®¹ä¸ºç©º
        empty_after_tags_mask = cleaned_df['cleaned_text'].str.strip() == ''
        cleaned_df.loc[empty_after_tags_mask & cleaned_df['is_valid'], 'invalidation_reason'] = 'empty_after_tags_clean'
        cleaned_df.loc[empty_after_tags_mask, 'is_valid'] = False
        # æ›´æ–°ä¹‹å‰çš„æ©ç ä»¥è®¡ç®—æ–°æ ‡è®°çš„æ¡ç›®
        previous_mask = mask if 'mask' in locals() else pd.Series(False, index=cleaned_df.index)
        previous_mask = previous_mask | empty_after_split_mask
        marked_count = (empty_after_tags_mask & ~previous_mask).sum()
        if marked_count > 0:
            logger.info(f"æ­¥éª¤ 3/6: æ¸…ç†å¾®åšæ ‡ç­¾åï¼Œæ ‡è®°äº† {marked_count} è¡Œå› å†…å®¹ä¸ºç©ºè€Œæ— æ•ˆã€‚")
        else:
            logger.info("æ­¥éª¤ 3/6: å®Œæˆå¾®åšæ ‡ç­¾æ¸…ç†ï¼ˆæˆ–æ— éœ€æ¸…ç†ï¼‰ã€‚")
    else:
        logger.info("æ­¥éª¤ 3/6: è·³è¿‡å¾®åšæ ‡ç­¾æ¸…ç†ï¼ˆæœªå¯ç”¨ï¼‰ã€‚")
        # ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„æ©ç 
        empty_after_tags_mask = pd.Series(False, index=cleaned_df.index)


    # 4. é€šç”¨æ–‡æœ¬æ¸…æ´— (URL, HTML, å¤šä½™ç©ºæ ¼ç­‰) - åº”ç”¨åˆ° cleaned_text
    try:
        cleaned_df['cleaned_text'] = cleaned_df['cleaned_text'].apply(clean_text)
        # æ£€æŸ¥æ˜¯å¦å› ä¸ºæ¸…æ´—å˜ä¸ºç©ºï¼Œå¹¶æ ‡è®°ä¸ºæ— æ•ˆ
        empty_after_clean_mask = cleaned_df['cleaned_text'].str.strip() == ''
        cleaned_df.loc[empty_after_clean_mask & cleaned_df['is_valid'], 'invalidation_reason'] = 'empty_after_basic_clean'
        cleaned_df.loc[empty_after_clean_mask, 'is_valid'] = False
        # æ›´æ–°ä¹‹å‰çš„æ©ç 
        previous_mask = previous_mask if 'previous_mask' in locals() else pd.Series(False, index=cleaned_df.index)
        previous_mask = previous_mask | empty_after_tags_mask
        marked_count = (empty_after_clean_mask & ~previous_mask).sum() # è®¡ç®—æœ¬æ¬¡æ–°å¢çš„æ— æ•ˆæ ‡è®°
        if marked_count > 0:
            logger.info(f"æ­¥éª¤ 4/6: åº”ç”¨åŸºç¡€æ–‡æœ¬æ¸…æ´—åï¼Œæ ‡è®°äº† {marked_count} è¡Œå› å†…å®¹ä¸ºç©ºè€Œæ— æ•ˆã€‚")
        else:
             logger.info("æ­¥éª¤ 4/6: å®ŒæˆåŸºç¡€æ–‡æœ¬æ¸…æ´—ã€‚")
    except NameError:
         logger.error("é”™è¯¯ï¼šclean_text å‡½æ•°æœªå®šä¹‰ã€‚è·³è¿‡åŸºç¡€æ–‡æœ¬æ¸…æ´—ã€‚")


    # 5. æ­£åˆ™å…³é”®å­—æ¸…æ´—æ— æ„ä¹‰å†…å®¹ (åŸºäº cleaned_text)
    if meaningless_patterns:
        combined_pattern = '|'.join(meaningless_patterns)
        try:
            meaningless_mask = cleaned_df['cleaned_text'].str.contains(combined_pattern, case=False, regex=True, na=False)
            # æ ‡è®°ä¸ºæ— æ•ˆ
            cleaned_df.loc[meaningless_mask & cleaned_df['is_valid'], 'invalidation_reason'] = 'meaningless_pattern'
            cleaned_df.loc[meaningless_mask, 'is_valid'] = False
            # æ›´æ–° previous_mask
            previous_mask = previous_mask | empty_after_clean_mask
            marked_count = (meaningless_mask & ~previous_mask).sum() # è®¡ç®—æœ¬æ¬¡æ–°å¢çš„æ— æ•ˆæ ‡è®°
            if marked_count > 0:
                logger.info(f"æ­¥éª¤ 5/6: æ ¹æ®æ— æ„ä¹‰å†…å®¹æ¨¡å¼æ ‡è®°äº† {marked_count} è¡Œä¸ºæ— æ•ˆã€‚")
            else:
                 logger.info("æ­¥éª¤ 5/6: æœªå‘ç°åŒ¹é…æ— æ„ä¹‰å†…å®¹æ¨¡å¼çš„è¡Œã€‚")
        except re.error as e:
            logger.error(f"é”™è¯¯ï¼šæä¾›çš„æ— æ„ä¹‰å†…å®¹æ­£åˆ™è¡¨è¾¾å¼æ— æ•ˆï¼š{e}ã€‚è·³è¿‡æ­¤æ­¥éª¤ã€‚")
    else:
        logger.info("æ­¥éª¤ 5/6: æœªæä¾›æ— æ„ä¹‰å†…å®¹æ¨¡å¼ï¼Œè·³è¿‡æ­¤è¿‡æ»¤ã€‚")
        # ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„æ©ç 
        meaningless_mask = pd.Series(False, index=cleaned_df.index)

    # 6. æ¸…æ´—åæ–‡æœ¬é•¿åº¦è¿‡æ»¤ (åŸºäº cleaned_text) - æ³¨æ„è¿™ä¸€æ­¥ç§»åˆ°æœ€åï¼Œä¿è¯åœ¨æ‰€æœ‰æ¸…æ´—åè¿›è¡Œ
    length_mask = cleaned_df['cleaned_text'].str.len() < min_text_length
    # æ ‡è®°ä¸ºæ— æ•ˆ
    cleaned_df.loc[length_mask & cleaned_df['is_valid'], 'invalidation_reason'] = f'text_length_<{min_text_length}'
    cleaned_df.loc[length_mask, 'is_valid'] = False
    # æ›´æ–° previous_mask
    previous_mask = previous_mask | meaningless_mask
    marked_count = (length_mask & ~previous_mask).sum() # è®¡ç®—æœ¬æ¬¡æ–°å¢çš„æ— æ•ˆæ ‡è®°
    if marked_count > 0:
        logger.info(f"æ­¥éª¤ 6/6: æ ¹æ®æœ€å°é•¿åº¦ ({min_text_length}) æ ‡è®°äº† {marked_count} è¡Œä¸ºæ— æ•ˆã€‚")
    else:
        logger.info(f"æ­¥éª¤ 6/6: æ‰€æœ‰è¡Œå‡æ»¡è¶³æœ€å°é•¿åº¦è¦æ±‚ã€‚")

    # --- æ¸…ç†å’Œæ€»ç»“ ---
    final_invalid_count = (~cleaned_df['is_valid']).sum()
    logger.info(f"é€šç”¨ç¤¾äº¤åª’ä½“æ•°æ®æ¸…æ´—ï¼ˆæ ‡è®°æ¨¡å¼ï¼‰å®Œæˆã€‚æ€»è¡Œæ•°: {original_count}, æ ‡è®°æ— æ•ˆè¡Œæ•°: {final_invalid_count}")

    return cleaned_df 